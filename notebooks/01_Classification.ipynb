{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CherpanovNazim/learn-llm/blob/main/notebooks/01_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFWuKlyc76eQ"
      },
      "source": [
        "\n",
        "> [GitHub Repo](https://github.com/CherpanovNazim/learn-llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTfpJzv_76eS"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wait ~3 min for installations\n",
        "%%time\n",
        "\n",
        "!pip install -qU openai==1.40.3 vllm==0.5.4 transformers==4.44.0"
      ],
      "metadata": {
        "id": "LeD-2ij_8qsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wait ~3 min for installations\n",
        "%%time\n",
        "\n",
        "# Load the default model\n",
        "DEFAULT_MODEL = {\"model\": \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\", \"api_base\": \"http://localhost:8000/v1\", \"api_key\": \"EMPTY\"}\n",
        "\n",
        "#run VLLM\n",
        "!nohup vllm serve {DEFAULT_MODEL['model']} --quantization awq --max-model-len=4096 > vllm.log &\n",
        "!tail -f vllm.log | grep -q \"Uvicorn running\" && echo \"Now you can start using the model\""
      ],
      "metadata": {
        "id": "TmlZUxpV89XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Explainer class\n",
        "import json\n",
        "import openai\n",
        "from IPython.display import display, Markdown, Code, update_display\n",
        "import random\n",
        "from openai import OpenAI\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "class Explainer:\n",
        "    def __init__(self, system_prompt:str = None) -> None:\n",
        "        if system_prompt is None:\n",
        "            system_prompt = \"You are assistant for IT specialist who doesn't know NLP.\"\n",
        "        self.basic_system_prompt = system_prompt\n",
        "\n",
        "        self.config = DEFAULT_MODEL\n",
        "\n",
        "        self.client_explainer = OpenAI(base_url=self.config['api_base'], api_key = self.config['api_key'])\n",
        "\n",
        "    def llm_call(self, prompts, additional_system_prompt=\"\"):\n",
        "        system_prompt = self.basic_system_prompt + additional_system_prompt\n",
        "\n",
        "        completion = self.client_explainer.chat.completions.create(\n",
        "            model=self.config[\"model\"],\n",
        "            temperature=0,\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": prompts}])\n",
        "        print(completion)\n",
        "        return completion.choices[0].message.content.strip()\n",
        "\n",
        "    def stream_llm_call(self, prompts, additional_system_prompt=\"\"):\n",
        "        system_prompt = self.basic_system_prompt + additional_system_prompt\n",
        "\n",
        "        completion = self.client_explainer.chat.completions.create(\n",
        "            model=self.config[\"model\"],\n",
        "            temperature=0,\n",
        "            stream=True,\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": prompts}])\n",
        "\n",
        "        for chunk in completion:\n",
        "            if chunk.choices[0].delta.content is not None:\n",
        "                yield chunk.choices[0].delta.content\n",
        "\n",
        "    def __call__(self, text, output_type=\"markdown\"):\n",
        "        if output_type == \"markdown\":\n",
        "            data = list()\n",
        "            display_id = display(Markdown(''), display_id=True).display_id\n",
        "            for chunk in self.stream_llm_call(text, \"Format your answer using Markdown.\"):\n",
        "                data.append(chunk)\n",
        "                update_display(Markdown(''.join(data)), display_id=display_id)\n",
        "        elif output_type == \"code\":\n",
        "            results = self.llm_call(text, \"Provide only Python code without any side comments.\")\n",
        "            results = results.split(\"```\")[1].split(\"```\")[0]\n",
        "            display(Code(results))\n",
        "        else:\n",
        "            raise Exception(\"Unknown output type. Default: markdown, code\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eFS1dZiu9a3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain = Explainer()\n",
        "\n",
        "# use this class if you want to get some explanations\n",
        "# It uses LLAMA 3 to generate explanations :)"
      ],
      "metadata": {
        "id": "KbuEIXRx9Vym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgdXX1_o76eS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "\n",
        "# Set the base URL and API key\n",
        "# For production apps it's preferable to use some secret management system and don't store the key in git repo :)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url = DEFAULT_MODEL['api_base'],\n",
        "    api_key = DEFAULT_MODEL['api_key']\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yheSXTxG76eT"
      },
      "source": [
        "# Load dataset\n",
        "This is a sample from [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)\n",
        "\n",
        "Table contains:\n",
        "* ProductId - Unique identifier for the product\n",
        "* Score - Rating between 1 and 5\n",
        "* Text - Text of the review"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the Amazon Food Reviews dataset from Kaggle\n",
        "!pip install -q kaggle\n",
        "!kaggle datasets download -d snap/amazon-fine-food-reviews\n",
        "!unzip amazon-fine-food-reviews.zip"
      ],
      "metadata": {
        "id": "UK9zNkZr_Vyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mv3Bnxz76eT"
      },
      "outputs": [],
      "source": [
        "#Load the dataset\n",
        "reviews_df = pd.read_csv('Reviews.csv')\n",
        "reviews_df = reviews_df.sample(5000, random_state=42)\n",
        "reviews_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VWo2rQi76eU"
      },
      "outputs": [],
      "source": [
        "reviews_df.Score.value_counts().plot.barh(title='Score distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj9s1HEg76eU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# for small datasets of course better to use KFold CV, but for education purposes we will use train_test_split\n",
        "train_df, test_df = train_test_split(reviews_df, stratify=reviews_df.Score, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m890RJre76eV"
      },
      "source": [
        "## Baseline TF-IDF + Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmOeV1HM76eV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "clf = make_pipeline(TfidfVectorizer(), LogisticRegression(class_weight='balanced'))\n",
        "\n",
        "clf.fit(train_df['Text'], train_df['Score'])\n",
        "\n",
        "print('TF-IDF + LogReg Accuracy:', accuracy_score(test_df['Score'], clf.predict(test_df['Text'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dhnFPMH76eV"
      },
      "outputs": [],
      "source": [
        "def text_completion(prompt, temperature=0, max_tokens=2, return_completion_only=True, **kwargs):\n",
        "    completion_response = client.completions.create(\n",
        "                            model=DEFAULT_MODEL[\"model\"],\n",
        "                            temperature=temperature,\n",
        "                            max_tokens=max_tokens,\n",
        "                            prompt=prompt,\n",
        "                            **kwargs)\n",
        "    if return_completion_only:\n",
        "        return completion_response.choices[0].text.strip()\n",
        "    else:\n",
        "        return completion_response\n",
        "\n",
        "def chat_completion(prompts, temperature=0, max_tokens=2, system_prompt: str = None, **kwargs):\n",
        "    if system_prompt is None:\n",
        "        system_prompt = \"Just follow user instructions and don't communicate like \\\"Sure!\\\" or \\\"I hope this helps\\\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL[\"model\"],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                  {\"role\": \"user\", \"content\": prompts}],\n",
        "        **kwargs\n",
        "        )\n",
        "    return completion.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEK55bIs76eV"
      },
      "source": [
        "## Zero-shot Classification\n",
        "\n",
        "* We'll first assess the performance of the base models at classifying using a simple prompt.\n",
        "* If model starts generate some garbage, we will return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZh_IEHZ76eV"
      },
      "outputs": [],
      "source": [
        "explain('What is Zero-shot classification in LLMs like ChatGPT ? Make a short answer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COF3bGSw76eW"
      },
      "outputs": [],
      "source": [
        "def llm_predict(review_text, template):\n",
        "    prompt = template.format(review_text=review_text)\n",
        "    prediction = chat_completion(prompt).lower().strip()\n",
        "\n",
        "    try:\n",
        "        return int(prediction)\n",
        "    except:\n",
        "        # if model generates some garbage, we will return -1\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lml0kraV76eW"
      },
      "outputs": [],
      "source": [
        "zero_shot_template = \"\"\"\\\n",
        "You will get a review and you should predict the score of the review.\n",
        "Score is a number from 1 to 5. Answer with only with one number and nothing else.\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "Score:\n",
        "\"\"\"\n",
        "\n",
        "zero_shot_predictions = list(map(partial(llm_predict, template=zero_shot_template), tqdm(test_df.Text)))\n",
        "\n",
        "print('Accuracy:', accuracy_score(test_df['Score'], zero_shot_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKS8Vv7R76eW"
      },
      "outputs": [],
      "source": [
        "pd.Series(zero_shot_predictions).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBghdeNM76eW"
      },
      "source": [
        "* Using just a description of what we want to get as a prompt (w/o any training data) we already beat the TF-IDF+LogReg supervised baseline\n",
        "* Please notice that model sometime not following instructions and generating some garbage (-1)\n",
        "* It's because sometimes model is not fully following instructions (GPT4 for example do it much better)\n",
        "* Fortunately we can can force LLM to understand follow structure we want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kPGME0N76eW"
      },
      "source": [
        "## Force output structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxu1oc-l76eW"
      },
      "outputs": [],
      "source": [
        "zero_shot_structural_template = \"\"\"\\\n",
        "You will get a review and you should predict the score of the review.\n",
        "Score is a number from 1 to 5. Answer with only with one number and nothing else.\n",
        "\n",
        "Review: \"Very nice!\"\n",
        "Score: 5\n",
        "\n",
        "Review: \"Garbage!\"\n",
        "Score: 1\n",
        "\n",
        "Review: \"Kind of okay\"\n",
        "Score: 3\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "Score:\n",
        "\"\"\"\n",
        "\n",
        "zero_shot_structural_predictions = list(map(partial(llm_predict, template=zero_shot_structural_template), tqdm(test_df.Text)))\n",
        "\n",
        "print('Accuracy:', accuracy_score(test_df['Score'], zero_shot_structural_predictions))\n",
        "\n",
        "# check for garbage predictions\n",
        "assert all([x != -1 for x in zero_shot_structural_predictions]), 'There are some garbage predictions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIigPrNX76eW"
      },
      "source": [
        "* Nice! We've got even better accuracy!\n",
        "* Model doesn't generate garbage anymore\n",
        "* We just added a few synthetic examples to help LLM understand structure we want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MmhAOEB76eW"
      },
      "source": [
        "## [Chain of Thought](https://learnprompting.org/docs/intermediate/chain_of_thought) for Zero-shot Classification\n",
        "* It's well known that if you ask LLM to reason before commiting to answer it can provide better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTnY2tCt76eX"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\\\n",
        "You will get a review and you should predict the score of the review.\n",
        "Score is a number from 1 to 5. Answer with only with one number and nothing else.\n",
        "Try to give a short explanation before providing score.\n",
        "\n",
        "------------------\n",
        "Review: \"Very nice!\"\n",
        "Reasoning: ...\n",
        "Score: 5\n",
        "\n",
        "Review: \"Garbage!\"\n",
        "Reasoning: ...\n",
        "Score: 1\n",
        "\n",
        "Review: \"Kind of okay\"\n",
        "Reasoning: ...\n",
        "Score: 3\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "Reasoning: \"\"\"\n",
        "\n",
        "def llm_predict(review_text):\n",
        "    prompt = prompt_template.format(review_text=review_text)\n",
        "\n",
        "    # Step 1: generate some reasoning\n",
        "    reasoning = ''\n",
        "    while len(reasoning)<10:\n",
        "        # try to generate till we get desired reasoning structure\n",
        "        # we use stop keyword in order to stop generation if model tries to answer right away, before generating reasoning\n",
        "        # we also stop generation if model starts repeating original review text\n",
        "        reasoning = chat_completion(prompt, max_tokens=100, stop=['score:', 'Score:', review_text], temperature=0.7).strip().replace('\\n', '')\n",
        "\n",
        "    # Step 2: join original prompt with generate reasoning and ask to generate final score\n",
        "    # It's not necessary to have two steps, but decomposing it we can get more control over generation (especially for open source LLMs)\n",
        "    # Note: we use text_completion API, but for GPT3.5 and GPT4 it's not required\n",
        "    final_prompt = prompt + reasoning + '\\nScore: '\n",
        "    final_answer = text_completion(final_prompt, max_tokens=2).strip()\n",
        "\n",
        "    try:\n",
        "        return int(final_answer)\n",
        "    except:\n",
        "        # if model starts to generate some garbage, we will return -1\n",
        "        return -1\n",
        "\n",
        "chain_of_thought_predictions = list(map(llm_predict, tqdm(test_df.Text)))\n",
        "\n",
        "# Accuracy may wary since we use non-zero temperature for generation\n",
        "print('Accuracy:', accuracy_score(test_df['Score'], chain_of_thought_predictions))\n",
        "\n",
        "# # check for garbage predictions\n",
        "assert all([x != -1 for x in chain_of_thought_predictions]), 'There are some garbage predictions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5iTa4pp76eX"
      },
      "source": [
        "* We've got ~4% absolute improvement in accuracy using CoT\n",
        "* Ufortunately, generation process is much slower (~10x) since you need to generate reasoning before it can generate final answer\n",
        "* It's possible to provide more precise instuctions on way of reasoning for your specific task\n",
        "* You also can provide examples of reasoning to help LLM understand the way of reasoning you want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnJfcYYm76eX"
      },
      "source": [
        "# CoT with Self-Consistency\n",
        "\n",
        "* It's possible to even further improve accuracy by using [Self-Consistency](https://learnprompting.org/docs/intermediate/self_consistency) trick, but it also will increase generation time\n",
        "* It's basically an ensembles where you use majority vote\n",
        "* It will be N time slower than using CoT alone - where N number of runs\n",
        "\n",
        "> <br>**Important:** you need to use temperature > 0 to get some variance in generated answers\n",
        ">\n",
        "> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z3_T3M876eX"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def self_consitency(review_text, nb_generations=3):\n",
        "    # we generate predictions several times with CoT\n",
        "    # make sure that you use temperature>0 for generation\n",
        "    # then just take the most common prediction\n",
        "    results = [llm_predict(review_text) for _ in range(nb_generations)]\n",
        "    final_result = Counter(results).most_common()[0][0]\n",
        "    print('Predictions:', results, 'Final prediction:', final_result)\n",
        "    return final_result\n",
        "\n",
        "# just illustrate how it works\n",
        "chain_of_thought_self_cons_predictions = list(map(self_consitency, tqdm(test_df.Text.iloc[:5])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MXldfCR76eX"
      },
      "source": [
        "## Few-shot learning\n",
        "* Now we'll try to use few-shot learning to futher improve accuracy\n",
        "* LLMs can do In-Context-Learning [(ICL)](https://thegradient.pub/in-context-learning-in-context/) -  LLM learns to solve a new task at inference time (without any change to its weights) by being fed a prompt with examples of that task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swqSmoyG76eX"
      },
      "outputs": [],
      "source": [
        "def sample_few_shot_examples(dataset: pd.DataFrame, samples_per_class=1, seed=None):\n",
        "\n",
        "    # sample examples from each class\n",
        "    examples = dataset.groupby('Score').apply(lambda x: x.sample(samples_per_class, random_state=seed), include_groups=False)\n",
        "\n",
        "    # shuffle sampled examples\n",
        "    examples = examples.sample(frac=1, random_state=seed)\n",
        "\n",
        "    # construct final string\n",
        "    string = ''\n",
        "    for _, row in examples.iterrows():\n",
        "        review = str(row.Text).replace('\\n','')\n",
        "        string += f'Review: \"{review}\"\\nScore: {_[0]}\\n\\n'\n",
        "\n",
        "    return string.strip()\n",
        "\n",
        "sampled_few_shot_examples = sample_few_shot_examples(train_df, samples_per_class=1, seed=0)\n",
        "print(sampled_few_shot_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq9a2xFD76eX"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\\\n",
        "You will get a review and you should predict the score of the review.\n",
        "Score is a number from 1 to 5. Answer with only with one number and nothing else.\n",
        "\n",
        "------------------\n",
        "{few_shot_examples}\n",
        "\n",
        "Review: \"{review_text}\"\n",
        "Score:\"\"\"\n",
        "\n",
        "def llm_predict(review_text, few_shot_examples, llm_kwargs={}):\n",
        "    prompt = prompt_template.format(review_text=review_text, few_shot_examples=few_shot_examples)\n",
        "\n",
        "    # Note: that we use text-completion API for open-source LLMs, but for GPT it's not required\n",
        "    prediction = text_completion(prompt, **llm_kwargs).lower().strip()\n",
        "\n",
        "    try:\n",
        "        return int(prediction)\n",
        "    except:\n",
        "        # if model starts to generate some garbage, we will return -1\n",
        "        return -1\n",
        "\n",
        "few_shot_predictions = list(map(partial(llm_predict, few_shot_examples=sampled_few_shot_examples), tqdm(test_df.Text)))\n",
        "\n",
        "print('Accuracy:', accuracy_score(test_df['Score'], few_shot_predictions))\n",
        "\n",
        "# check for garbage predictions\n",
        "assert all([x != -1 for x in few_shot_predictions]), 'There are some garbage predictions'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnR4H2Q476eX"
      },
      "source": [
        "* By using only 5 training examples (one per class) from real data we've got Accuracy which is comparable to CoT results\n",
        "* But computational time is significantly lower (almost 4x lower) than CoT and bit higher that Zero-shot\n",
        "* * Generation of new tokens is much slower (and expensive) than enlarging size of the prompt\n",
        "* Selected few-shot examples significantly affect accuracy, so it's important to select them carefully\n",
        "* It's well known that even order of sampled examples also affect accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkdnHxd-76eX"
      },
      "source": [
        "## Classes (tokens) probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOz331rb76eX"
      },
      "source": [
        "* We can also use LLMs to get probability of each class (token) in the prompt\n",
        "* It can be used to get some insights about unsertainty of the LLM's predictions\n",
        "* For that we we need to pass [logprobs](https://platform.openai.com/docs/api-reference/completions/create#completions/create-logprobs) parameter to API call\n",
        "* Usually it's better to have classes which are represented by only one token (like in our case)\n",
        "* In real use cases it's better to use [logit_bias](https://platform.openai.com/docs/api-reference/completions/create#completions/create-logit_bias) as well - to maximize probability that model will return in logprobs all the classes we want - unfortunately it's not implemented in VLLM yet which we use for our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qnvxI7H76eY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_tokens_probability(raw_prediction, classes_tokens=None):\n",
        "    # extract logprobs from API answer\n",
        "    logpropbs = raw_prediction.choices[0].logprobs.top_logprobs\n",
        "\n",
        "    # the first predicted token can be a space, not the class token itself\n",
        "    # so we need to find which token contains the most amount of potential tokens\n",
        "    classes_tokens_intersect = [len(set(token.keys()) & set(classes_tokens)) for token in logpropbs]\n",
        "    logpropbs_token_pos = logpropbs[np.argmax(classes_tokens_intersect)]\n",
        "\n",
        "    # then in order to convert that into probabilities we need to exponentiate and normalize\n",
        "    # potentially there is chance that som of classes are not presented in the logprobs, so we use -1000 (low probability)\n",
        "    classes_prob_unnorm = [np.exp(logpropbs_token_pos.get(c, -1000)) for c in classes_tokens]\n",
        "    classes_prob = np.array(classes_prob_unnorm) / np.sum(classes_prob_unnorm)\n",
        "\n",
        "    # and finally return classes (tokens) probabilities\n",
        "    return {c: p for c, p in zip(classes_tokens, classes_prob)}\n",
        "\n",
        "\n",
        "\n",
        "results = list()\n",
        "for review_text in tqdm(test_df.Text.iloc[:10]):\n",
        "    prompt = prompt_template.format(review_text=review_text, few_shot_examples=sampled_few_shot_examples)\n",
        "\n",
        "    # we use param logprobs which will return logprob of tokens for each position\n",
        "    prediction = text_completion(prompt, return_completion_only=False, logprobs=10)\n",
        "\n",
        "    # extract probabilities for each class\n",
        "    classes_prob = get_tokens_probability(prediction, classes_tokens=list(map(str, range(1, 6))))\n",
        "    results.append(classes_prob)\n",
        "\n",
        "# show class probability for each review\n",
        "# columns - our classes, rows - reviews\n",
        "pd.DataFrame.from_records(results).round(2).style.background_gradient(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA4_N-SA76eY"
      },
      "source": [
        "## Embeddings and LogReg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV_l1gSX76eY"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers==3.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFDSXvWE76eY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# (!) It will download 2GB neural network and run inference on your device\n",
        "# Long texts will be truncated to at most 512 tokens.\n",
        "model = SentenceTransformer('intfloat/e5-large-v2')\n",
        "\n",
        "train_embeddings = model.encode(('query: ' + train_df.Text).to_list())\n",
        "test_embeddings = model.encode(('query: ' + test_df.Text).to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxPDMVN976eY"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(train_embeddings, train_df['Score'])\n",
        "\n",
        "print('E5 Large embeddings + LogReg Accuracy:',\n",
        "      accuracy_score(test_df['Score'], clf.predict(test_embeddings)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOAwYnWC76eY"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ0cFUSw76eY"
      },
      "source": [
        "| Approach             | Accuracy | Training data        |\n",
        "|----------------------|----------|----------------------|\n",
        "| TF-IDF + LogReg      | 0.604    | Full (2187 examples) |\n",
        "| Zero-Shot naive      | 0.720    | No                   |\n",
        "| Zero-Shot structured | 0.732    | No                   |\n",
        "| Zero-Shot with CoT   | 0.741    | No                   |\n",
        "| Few-Shot             | 0.757    | 5 examples           |\n",
        "| E5 + LogReg          | 0.749    | Full (2187 examples) |\n",
        "\n",
        "* If you don't have training data at all - you can use Zero-Shot approach\n",
        "* If you have some training data (at least 1 sample per class) - you can use Few-Shot approach\n",
        "* CoT can be used to improve accuracy of Zero-Shot approach as well as Few-Shot\n",
        "* You can use LLM to get probability of each class (token)\n",
        "* You can also use LLM to label your data -> train more classical models (like E5 + LogReg) -> use it for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYPeryfL76eY"
      },
      "source": [
        "# Homework\n",
        "* Try to play with prompt in [Force output structure](#Force-output-structure) cell to get better accuracy e.g. provide more context on task\n",
        "* Try to sample different \\ more examples per class in [Few-shot learning](#Few-shot-learning) cell and see how it affects accuracy\n",
        "* * Extra: Try to find the best set of examples from training set using validation set (not test set)\n",
        "* Implement self-consistency trick for [Few-shot learning](#Few-shot-learning) cell\n",
        "* Try other models (like Mistral 7B) from other config files\n",
        "* TODO: Add one more dataset for multi-class classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bMe_gOF76eY"
      },
      "source": [
        "# Extra\n",
        "* There are libraries which can help you to use LLMs\n",
        "* * [guidance](https://github.com/guidance-ai/guidance) - enables you to control modern LLMs more efficiently than traditional prompting or chaining\n",
        "* * [DSPy](https://github.com/stanfordnlp/dspy/tree/main) - provides composable and declarative modules for instructing LMs in a familiar Pythonic syntax"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}