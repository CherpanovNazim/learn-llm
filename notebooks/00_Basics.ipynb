{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CherpanovNazim/learn-llm/blob/main/notebooks/00_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac8f20d",
      "metadata": {
        "id": "0ac8f20d"
      },
      "source": [
        "* [GitHub Repo](https://github.com/CherpanovNazim/learn-llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "61372fac",
      "metadata": {
        "id": "61372fac"
      },
      "outputs": [],
      "source": [
        "#clone git repository\n",
        "!git clone -q https://github.com/CherpanovNazim/learn-llm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5af19ed4",
      "metadata": {
        "id": "5af19ed4",
        "outputId": "51fae99b-2a1c-46d3-c492-7fe057dd824a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m360.7/360.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.9/75.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCPU times: user 1.54 s, sys: 216 ms, total: 1.76 s\n",
            "Wall time: 3min 10s\n"
          ]
        }
      ],
      "source": [
        "# wait ~3 min for installations\n",
        "%%time\n",
        "\n",
        "!pip install -qU openai==1.40.3 vllm==0.5.4 transformers==4.44.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "LbYR367oX3i-",
      "metadata": {
        "id": "LbYR367oX3i-",
        "outputId": "bfc4aa5f-acf0-4674-dca9-0fba1c599261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n",
            "Now you can start using the model\n",
            "CPU times: user 661 ms, sys: 92 ms, total: 753 ms\n",
            "Wall time: 1min 49s\n"
          ]
        }
      ],
      "source": [
        "# wait ~2 min for installations\n",
        "%%time\n",
        "\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# Load the default model\n",
        "DEFAULT_MODEL = json.load(open('learn-llm/configs/llama_3_8B_instruct_awq.json', 'r'))\n",
        "\n",
        "#run VLLM\n",
        "!nohup vllm serve {DEFAULT_MODEL['model']} --quantization awq --max-model-len=4096 > vllm.log &\n",
        "!tail -f vllm.log | grep -q \"Uvicorn running\" && echo \"Now you can start using the model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b064a83c",
      "metadata": {
        "id": "b064a83c"
      },
      "source": [
        "# Why LLAMA 3 ?\n",
        "\n",
        "* We mainly use GPT 3.5 or GPT 4 models for production use-cases\n",
        "* For this course we use LLAMA 3 8B model for educational purposes\n",
        "* LLAMA 3 is smaller, but still very powerful\n",
        "* We use OpenAI-compatible API for our LLAMA 3 - so this should would work for GPT 3.5 or GPT 4 as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "sVYOdeh0ZQ_v",
      "metadata": {
        "id": "sVYOdeh0ZQ_v",
        "outputId": "cda3815d-d8f9-44bb-874f-7704b389adba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/learn-llmnotebooksutilsexplainer.py': [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Comparison of LLMs and Other Deep Learning NLP Models**\n\n| **Model Type** | **Description** | **Key Characteristics** | **Primary Use Case** |\n| --- | --- | --- | --- |\n| **LLMs (Large Language Models)** | Pre-trained models that learn general language patterns and relationships | Auto-regressive architecture, self-supervised learning, massive parameter count | Conversational AI, text generation, language translation |\n| **Transformers** | Encoder-decoder architecture for sequence-to-sequence tasks | Self-attention mechanism, parallel processing, fixed input sequence length | Machine translation, text summarization, question answering |\n| **RNNs (Recurrent Neural Networks)** | Recurrent architecture for sequential data | Hidden state, recurrent connections, fixed sequence length | Speech recognition, sentiment analysis, language modeling |\n| **CNNs (Convolutional Neural Networks)** | Convolutional architecture for grid-like data | Local connections, shared weights, fixed spatial resolution | Text classification, sentiment analysis, topic modeling |\n| **BERT (Bidirectional Encoder Representations from Transformers)** | Pre-trained model for contextualized language understanding | Masked language modeling, next sentence prediction, contextualized embeddings | Question answering, sentiment analysis, text classification |\n\nNote: This table provides a high-level comparison of the key characteristics and primary use cases of each model type. LLMs are a specific type of model that encompasses the characteristics of other models, but they are not mutually exclusive."
          },
          "metadata": {}
        }
      ],
      "source": [
        "!python3 learn-llm\\notebooks\\utils\\explainer.py\n",
        "sys.path.append('learn-llm/notebooks/utils')\n",
        "\n",
        "from explainer import Explainer\n",
        "\n",
        "explain = Explainer(DEFAULT_MODEL)\n",
        "# use this class if you want to get some explanations\n",
        "\n",
        "explain(\"difference between LLMs and other deep learning NLP models in a small table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e0711f6f",
      "metadata": {
        "id": "e0711f6f"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Set the base URL and API key\n",
        "# For production apps it's preferable to use some secret management system and don't store the key in git repo :)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url = DEFAULT_MODEL['api_base'],\n",
        "    api_key = DEFAULT_MODEL['api_key']\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d19e0b-73ea-45b9-aa45-60e597704dca",
      "metadata": {
        "id": "d8d19e0b-73ea-45b9-aa45-60e597704dca"
      },
      "source": [
        "## Completion VS ChatCompletion API:  \n",
        "There is also 2 variants of API or how to use these models, for example in Python: `Completion` and `ChatCompletion`.  \n",
        "\n",
        "## [Completion API](https://platform.openai.com/docs/guides/gpt/completions-api):  \n",
        "For the most common case you need to specify:\n",
        "* model name\n",
        "* prompt that will have the task for the model and should be a string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0ba43208",
      "metadata": {
        "id": "0ba43208"
      },
      "outputs": [],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0,\n",
        "    max_tokens=128,\n",
        "    n=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5b3c17ec-304b-4ddc-a6f3-4a09e02fe00b",
      "metadata": {
        "id": "5b3c17ec-304b-4ddc-a6f3-4a09e02fe00b",
        "outputId": "7063f92c-5b25-4aee-883b-6098e10ba57a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion(id='cmpl-725bc747d1aa4c8ca3ae2b0229884f23', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' Here are a few ideas to get you started:\\n* \"Cool treats for hot days\"\\n* \"Scoops of happiness\"\\n* \"The cream of the crop\"\\n* \"Chill out with us\"\\n* \"A scoop above the rest\"\\n* \"Sweet treats for sweet people\"\\n* \"The coolest place in town\"\\n* \"A taste of paradise\"\\n* \"Indulge in the sweet life\"\\n* \"A scoop of fun for everyone\"\\n* \"The perfect treat for any occasion\"\\n* \"Cool, creamy, and delicious\"\\n* \"A sweet escape from the ordinary\"\\n* \"The best ice cream in town,', stop_reason=None)], created=1723717898, model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=128, prompt_tokens=11, total_tokens=139))\n"
          ]
        }
      ],
      "source": [
        "# Example of an output as an completion object:\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8658708a-2bc1-4aab-a831-92b9a4d33abd",
      "metadata": {
        "id": "8658708a-2bc1-4aab-a831-92b9a4d33abd",
        "outputId": "f71a4ff0-71fa-46ce-b4d2-5ae6b8ce8b75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few ideas to get you started:\n",
            "* \"Cool treats for hot days\"\n",
            "* \"Scoops of happiness\"\n",
            "* \"The cream of the crop\"\n",
            "* \"Chill out with us\"\n",
            "* \"A scoop above the rest\"\n",
            "* \"Sweet treats for sweet people\"\n",
            "* \"The coolest place in town\"\n",
            "* \"A taste of paradise\"\n",
            "* \"Indulge in the sweet life\"\n",
            "* \"A scoop of fun for everyone\"\n",
            "* \"The perfect treat for any occasion\"\n",
            "* \"Cool, creamy, and delicious\"\n",
            "* \"A sweet escape from the ordinary\"\n",
            "* \"The best ice cream in town,\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e009b007-9b37-4deb-9605-91b4be5fee58",
      "metadata": {
        "id": "e009b007-9b37-4deb-9605-91b4be5fee58"
      },
      "source": [
        "Let's cover most important parameters for API:  \n",
        "- `temperature` -- aka randomness or 'creativity' of an output, `[0, 1]`. 0 -- no random, the output will be the same all the executions. 1 -- the output will be very different each execution.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b50c212a-0b03-4a0d-820f-ec9ae34258d7",
      "metadata": {
        "id": "b50c212a-0b03-4a0d-820f-ec9ae34258d7",
        "outputId": "939eaac6-7948-41ac-a9d0-7c5109a8a1dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few ideas to get you started. \"Cool Treats Anytime\" or \"Sweet Escape in Every Scoop.\" Here are a few more suggestions: \"Taste the Chill,\" \"Where the Flavor Never Melts,\" \"Endless Choices, Endless Joy,\" or \"Beat the Heat with a Sweet Treat.\" Which one do you think captures the playful and fun atmosphere of an ice cream shop?\n",
            "Ice Cream Shop Tagline Suggestions:\n",
            "Taste the Chill\n",
            "Where the Flavor Never Melts\n",
            "Endless Choices, Endless Joy\n",
            "Beat the Heat with a Sweet Treat\n",
            "Cool Treats Anytime\n",
            "Sweet Escape in Every\n"
          ]
        }
      ],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=128,\n",
        "    n=1,\n",
        ")\n",
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066a3580-95ff-4c61-853e-c35a5295f76e",
      "metadata": {
        "id": "066a3580-95ff-4c61-853e-c35a5295f76e"
      },
      "source": [
        "* `max_tokens` -- limits number of tokens in the output. It works like too low values will shrink your output, but not making the model to generate very short output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "bd5b020d-4b5b-4aef-951e-32147d3f49f0",
      "metadata": {
        "id": "bd5b020d-4b5b-4aef-951e-32147d3f49f0",
        "outputId": "13a262de-969e-4e50-f327-b50556e4c346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creamy Delights\n"
          ]
        }
      ],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=5,\n",
        "    n=1,\n",
        ")\n",
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "060a6675-38e5-434b-8882-9b972f35a871",
      "metadata": {
        "id": "060a6675-38e5-434b-8882-9b972f35a871"
      },
      "source": [
        "* `n` -- how many output variants to generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4850b1ad-4e19-4066-8300-7ef34f320c20",
      "metadata": {
        "id": "4850b1ad-4e19-4066-8300-7ef34f320c20"
      },
      "outputs": [],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=128,\n",
        "    n=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "94169ea9",
      "metadata": {
        "id": "94169ea9",
        "outputId": "888a13d5-a3af-49ec-ca19-388422ee3c73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few options to consider:\n",
            "- \"Cool Treats, Warm Smiles\"\n",
            "- \"The Coolest Treat in Town\"\n",
            "- \"Chill Out with Us\"\n",
            "- \"Scoop Up a Smile\"\n",
            "- \"Freeze Your Worries Away\"\n",
            "- \"Sweet Treats Guaranteed\"\n",
            "- \"A Chill Breeze in Every Bite\"\n",
            "- \"Melt Your Heart with Our Ice Cream\"\n",
            "Which tagline do you think is the most effective? Why?\n",
            "I think the most effective tagline is \"Melt Your Heart with Our Ice Cream.\" This tagline is most effective because it uses a playful and whimsical turn of phrase\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "60ded41a",
      "metadata": {
        "id": "60ded41a",
        "outputId": "9cca4a28-0203-4684-811c-c046791bf33a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Note: I can provide you with the name of the shop later. For now, let's focus on a tagline.) Here are some ideas to get you started:\n",
            "\n",
            "*   A phrase that evokes a sense of fun and adventure, like \"Savor the Sweet Escape\" or \"Indulge in the Art of Joy.\"\n",
            "*   A tagline that highlights the unique flavors and ingredients, such as \"Handcrafted Happiness in Every Bite\" or \"Experience the Taste of the World in Every Scoop.\"\n",
            "*   A phrase that emphasizes the quality and care that goes into each and every scoop, like \"Where Every Scoop\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[1].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39b39b8-1b8f-4e01-acc4-f89d2bfcbdd4",
      "metadata": {
        "id": "e39b39b8-1b8f-4e01-acc4-f89d2bfcbdd4"
      },
      "source": [
        "Be aware, it's meaningless to use `n > 1` and `temperature == 0` since you will get `n` equal results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd35cf0-4221-4f8f-bc60-d1b4adaa1bfa",
      "metadata": {
        "id": "6fd35cf0-4221-4f8f-bc60-d1b4adaa1bfa"
      },
      "source": [
        "Other parameters you can check on official [OpenAI page](https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens:~:text=given%20chat%20conversation.-,Request%20body,-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5932a791-66c6-4170-a882-8a0f62466ebd",
      "metadata": {
        "id": "5932a791-66c6-4170-a882-8a0f62466ebd"
      },
      "source": [
        "## [ChatCompletion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api):  \n",
        "\n",
        "For the most common case you need to specify:\n",
        "* model name\n",
        "* messages in the form of list of dictionaries; they will emulate a chat between a user and an assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9c970684-b402-4c3f-a06a-54847feb19c0",
      "metadata": {
        "id": "9c970684-b402-4c3f-a06a-54847feb19c0"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=DEFAULT_MODEL[\"model\"],\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "35698270-1d09-4247-b790-2ea51dc506f1",
      "metadata": {
        "id": "35698270-1d09-4247-b790-2ea51dc506f1",
        "outputId": "b7523a53-90b9-4b47-b2d2-4009ac2e00d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chat-cdbcc38318874d2ca7883d6c59d36fea', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at the Globe Life Field in Arlington, Texas, with the 2020 World Series between the Los Angeles Dodgers and the Tampa Bay Rays happening there.', refusal=None, role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1723717972, model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=79, total_tokens=118))\n"
          ]
        }
      ],
      "source": [
        "# Example of an output completion object:\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "7d9403ca-f718-4f3b-bf84-482e2c4fb81f",
      "metadata": {
        "id": "7d9403ca-f718-4f3b-bf84-482e2c4fb81f",
        "outputId": "0a92dfe9-21b9-46db-c7f5-a5f4f49da298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2020 World Series was played at the Globe Life Field in Arlington, Texas, with the 2020 World Series between the Los Angeles Dodgers and the Tampa Bay Rays happening there.\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104baad5-b52f-4a55-8c38-339ebe8aea65",
      "metadata": {
        "id": "104baad5-b52f-4a55-8c38-339ebe8aea65"
      },
      "source": [
        "The **role** here is specifying who exactly sends the message, whether its from Human or from AI.  \n",
        "- **system role** is not required, but helpful to specify how a model should consider itself.  \n",
        "[From OpenAI documentation](https://platform.openai.com/docs/guides/gpt/chat-completions-api#:~:text=The%20system%20message,a%20helpful%20assistant.%22):  \n",
        "\"The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the modelâ€™s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"  \n",
        "- **human role** are possible inputs from the user helping a model to 'fit' on them\n",
        "- **assistant** -- examples of a desired behavior on the user's inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732f09c1-4c98-4e40-8e5d-939e4727fa1d",
      "metadata": {
        "id": "732f09c1-4c98-4e40-8e5d-939e4727fa1d"
      },
      "source": [
        "# ğŸ“ Prompt Engineering:  \n",
        "In this section we will cover:\n",
        "* Existing best techniques for prompt engineerging\n",
        "* Completion prompts for some Data Science tasks  \n",
        "\n",
        "\n",
        "*[References] This section is based on [DeepLearning.ai course](https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction?_gl=1*tjnub9*_ga*MzgyMDU1MDc3LjE2ODMwNTU2MzE.*_ga_PZF1GBS1R1*MTY4Nzk0NjM3My45LjEuMTY4Nzk0Nzk4Ni4zOC4wLjA.).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "38b8e7fb-57c5-439f-9397-6a8b31c84c4d",
      "metadata": {
        "id": "38b8e7fb-57c5-439f-9397-6a8b31c84c4d"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt: str, system_prompt: str = None):\n",
        "    if system_prompt is None:\n",
        "        system_prompt = \"Just follow user instructions and don't communicate like \\\"Sure!\\\" or \\\"I hope this helps\\\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "              {\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3a359a-566e-4af0-ba66-219e36a77283",
      "metadata": {
        "id": "6a3a359a-566e-4af0-ba66-219e36a77283"
      },
      "source": [
        "## Best practices:  \n",
        "Now let's observe and try different techniques helping you to construct a comprehensive and effective prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85a877b-4474-4d6f-a3a9-b3f8d2d20304",
      "metadata": {
        "id": "a85a877b-4474-4d6f-a3a9-b3f8d2d20304"
      },
      "source": [
        "**1. Use delimiters to indicate distinct parts of the input clearly:**\n",
        "\n",
        "Delimiters can be anything like: ```, \"\"\", < >,Â \\<tag> \\</tag>. It could help model focusing on specific parts of the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "be353bde-5609-4d1f-907c-46c40835790f",
      "metadata": {
        "id": "be353bde-5609-4d1f-907c-46c40835790f",
        "outputId": "0fe90ffc-6830-4ab8-bbd6-2b1c76aee304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jupiter is the largest planet in the Solar System, a gas giant known since ancient times and visible to the naked eye as one of the brightest objects in the night sky.\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "Jupiter is the fifth planet from the Sun and the largest in the Solar System.\n",
        "It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half\n",
        "times that of all the other planets in the Solar System combined.\n",
        "Jupiter is one of the brightest objects visible to the naked eye in the night sky,\n",
        "and has been known to ancient civilizations since before recorded history.\n",
        "It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be\n",
        "bright enough for its reflected light to cast visible shadows,[20] and is on average\n",
        "the third-brightest natural object in the night sky after the Moon and Venus.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Summarize the text delimited by triple backticks into a single sentence. ```{text}``` \"\"\"\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b42760-edf8-422a-9f91-c9ade8d2daca",
      "metadata": {
        "id": "e2b42760-edf8-422a-9f91-c9ade8d2daca"
      },
      "source": [
        "**2. Ask for a structured output:**\n",
        "\n",
        "You can as about JSON, HTML or any other reasonable format. JSON could be useful in many cases, let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "240f6033-c796-430f-a444-ff419ed16f0c",
      "metadata": {
        "id": "240f6033-c796-430f-a444-ff419ed16f0c",
        "outputId": "3a23f9d7-068f-42d7-8af7-afa644b97621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"books\": [\n",
            "    {\n",
            "      \"book_id\": 1,\n",
            "      \"title\": \"The Last Dreamer\",\n",
            "      \"author\": \"Ava Morales\",\n",
            "      \"genre\": \"Fantasy\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 2,\n",
            "      \"title\": \"The Memory Weaver\",\n",
            "      \"author\": \"Ethan Blackwood\",\n",
            "      \"genre\": \"Science Fiction\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 3,\n",
            "      \"title\": \"The Shadowlands\",\n",
            "      \"author\": \"Lila Grey\",\n",
            "      \"genre\": \"Horror\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three made-up book titles along\n",
        "with their authors and genres. Provide them in JSON\n",
        "format with the following keys: book_id, title, author, genre.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d4c9fa-6d5a-4705-a32d-f319a40646de",
      "metadata": {
        "id": "28d4c9fa-6d5a-4705-a32d-f319a40646de"
      },
      "source": [
        "**3. Ask the model to check whether conditions are satisfied:**  \n",
        "It's like `IF-ELSE` statement inside a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b04ccb70-b075-4290-b5fb-e63568e6f68b",
      "metadata": {
        "id": "b04ccb70-b075-4290-b5fb-e63568e6f68b",
        "outputId": "2d7d44c4-2f1b-4065-e0bf-323be3e5f3e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps provided:\n",
            " Step 1 - Get some water boiling.\n",
            "Step 2 - Grab a cup and put a tea bag in it.\n",
            "Step 3 - Once the water is hot enough, pour it over the tea bag.\n",
            "Step 4 - Let it sit for a bit so the tea can steep.\n",
            "Step 5 - After a few minutes, take out the tea bag.\n",
            "Step 6 - If you like, add some sugar or milk to taste.\n",
            "Step 7 - Enjoy your delicious cup of tea.\n",
            "\n",
            " ------------------------------ \n",
            "\n",
            "Steps missed:\n",
            " No steps provided.\n"
          ]
        }
      ],
      "source": [
        "recipe_w_steps = f\"\"\"\n",
        "Making a cup of tea is easy! First, you need to get some\n",
        " water boiling. While that's happening,\n",
        "grab a cup and put a tea bag in it. Once the water is\n",
        "hot enough, just pour it over the tea bag.\n",
        "Let it sit for a bit so the tea can steep. After a\n",
        "few minutes, take out the tea bag. If you\n",
        "like, you can add some sugar or milk to taste.\n",
        "And that's it! You've got yourself a delicious\n",
        "cup of tea to enjoy.\n",
        "\"\"\"\n",
        "\n",
        "recipe_wo_steps = f\"\"\"\n",
        "Making a cup of tea is easy! Just do it!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = \"\"\"\n",
        "You will be provided with text delimited by triple quotes. If it contains some instructions, re-write those instructions in the following format:\n",
        "Step 1 - ... Step 2 - ... Step N - ...\n",
        "\n",
        "If provided text does not contain any instructions, simply write \\\"No steps provided.\\\".\n",
        "\n",
        "Text: \\\"\\\"\\\"{recipe}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "print(\"Steps provided:\\n\", generate_response(prompt=prompt.format(recipe=recipe_w_steps)))\n",
        "print(\"\\n\", \"-\"*30, \"\\n\")\n",
        "print(\"Steps missed:\\n\", generate_response(prompt=prompt.format(recipe=recipe_wo_steps)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37bf1a7-8869-45af-aaa2-a2897a17939c",
      "metadata": {
        "id": "e37bf1a7-8869-45af-aaa2-a2897a17939c"
      },
      "source": [
        "**4. Focus on specific aspects:**  \n",
        "Specifically ask a model to pay more attention on some details according to the task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "90df4576-bdc8-47a2-ae07-c10ead40b45d",
      "metadata": {
        "id": "90df4576-bdc8-47a2-ae07-c10ead40b45d",
        "tags": [],
        "outputId": "aeb4f1c0-bf30-4cae-9d1c-474bd08e1425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This stylish office chair is crafted from high-quality materials, featuring a 10mm cast aluminum shell with a durable PA6/PA66 coating, paired with a HD36 foam seat and pneumatic adjust for comfortable use. Available in various configurations to suit home or business settings.\n"
          ]
        }
      ],
      "source": [
        "fact_sheet_chair = \"\"\"\n",
        "OVERVIEW\n",
        "- Part of a beautiful family of mid-century inspired office furniture,\n",
        "including filing cabinets, desks, bookcases, meeting tables, and more.\n",
        "- Several options of shell color and base finishes.\n",
        "- Available with plastic back and front upholstery (SWC-100)\n",
        "or full upholstery (SWC-110) in 10 fabric and 6 leather options.\n",
        "- Base finish options are: stainless steel, matte black,\n",
        "gloss white, or chrome.\n",
        "- Chair is available with or without armrests.\n",
        "- Suitable for home or business settings.\n",
        "- Qualified for contract use.\n",
        "\n",
        "CONSTRUCTION\n",
        "- 5-wheel plastic coated aluminum base.\n",
        "- Pneumatic chair adjust for easy raise/lower action.\n",
        "\n",
        "DIMENSIONS\n",
        "- WIDTH 53 CM | 20.87â€\n",
        "- DEPTH 51 CM | 20.08â€\n",
        "- HEIGHT 80 CM | 31.50â€\n",
        "- SEAT HEIGHT 44 CM | 17.32â€\n",
        "- SEAT DEPTH 41 CM | 16.14â€\n",
        "\n",
        "OPTIONS\n",
        "- Soft or hard-floor caster options.\n",
        "- Two choices of seat foam densities:\n",
        " medium (1.8 lb/ft3) or high (2.8 lb/ft3)\n",
        "- Armless or 8 position PU armrests\n",
        "\n",
        "MATERIALS\n",
        "SHELL BASE GLIDER\n",
        "- Cast Aluminum with modified nylon PA6/PA66 coating.\n",
        "- Shell thickness: 10 mm.\n",
        "SEAT\n",
        "- HD36 foam\n",
        "\n",
        "COUNTRY OF ORIGIN\n",
        "- Italy\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\n",
        "Write a product description based on the information provided in the technical specifications delimited by triple backticks.\n",
        "\n",
        "The description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.\n",
        "\n",
        "Use at most 50 words.\n",
        "Technical specifications: ```{fact_sheet_chair}```\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b93c94-6075-47b1-83dc-fe8b0d8c26cb",
      "metadata": {
        "id": "28b93c94-6075-47b1-83dc-fe8b0d8c26cb"
      },
      "source": [
        "**5. Specify the modelâ€™s role:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c95ca766-462b-413a-a8e6-0d1e7f91110f",
      "metadata": {
        "id": "c95ca766-462b-413a-a8e6-0d1e7f91110f",
        "outputId": "fd76ac5a-709a-488d-840d-cda6354a0f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Pizza Palace! I'll be happy to take your order today. What can I get for you?\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "You are OrderBot - an automated service to collect orders for a pizza restaurant.\n",
        "You first greet the customer, then collects the order, and then asks if it's a pickup or delivery.\n",
        "You wait to collect the entire order, then summarize it and check for a final time if the customer wants to add anything else.\n",
        "\"\"\"\n",
        "\n",
        "# Please note that we provided this prompt to system prompt argument\n",
        "print(generate_response(prompt='Hi!', system_prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35dec9e3-089a-437f-91d2-50ab80475ef5",
      "metadata": {
        "id": "35dec9e3-089a-437f-91d2-50ab80475ef5"
      },
      "source": [
        "## Prompt examples for Data Science / Machine Learning tasks:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0f847ed1-fecd-430d-a7bd-4482bf6002ba",
      "metadata": {
        "id": "0f847ed1-fecd-430d-a7bd-4482bf6002ba"
      },
      "outputs": [],
      "source": [
        "prod_review = \"\"\"\\\n",
        "Got this panda plush toy for my daughter's birthday, \\\n",
        "who loves it and takes it everywhere. It's soft and \\\n",
        "super cute, and its face has a friendly look. It's \\\n",
        "a bit small for what I paid though. I think there \\\n",
        "might be other options that are bigger for the \\\n",
        "same price. It arrived a day earlier than expected, \\\n",
        "so I got to play with it myself before I gave it \\\n",
        "to her.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a7cade3-e51f-4033-9156-bfd473c15dd9",
      "metadata": {
        "id": "0a7cade3-e51f-4033-9156-bfd473c15dd9"
      },
      "source": [
        "**1. Summarization with focus:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5fc02f0a-91b1-4239-94db-98a281ab52d7",
      "metadata": {
        "id": "5fc02f0a-91b1-4239-94db-98a281ab52d7",
        "outputId": "6e8f89c4-4582-495a-83e8-a27ca6b84fc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer felt the plush toy was small for its price, suggesting a potential value perception issue, and may consider larger options at the same price.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a short summary of a product review from an ecommerce site \\\n",
        "to give feedback to the pricing deparmtment, responsible for determining the price of the product.\n",
        "Summarize the review below in at most 30 words, and focusing on any aspects that are relevant to the price and perceived value.\n",
        "\n",
        "Review:\\n{prod_review}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61aeb86c-104f-4695-bc4b-b52a05684a07",
      "metadata": {
        "id": "61aeb86c-104f-4695-bc4b-b52a05684a07"
      },
      "source": [
        "**2. Information extraction:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de89fd7b-5098-4687-877b-90c6c430cc73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de89fd7b-5098-4687-877b-90c6c430cc73",
        "outputId": "8e698cf7-59e7-4a4a-96dd-5859e622e635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Delivery arrived a day earlier than expected, which was a pleasant surprise. However, the customer felt the product was a bit small for the price paid.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize the review below in order to give feedback to delivery department. Limit to 30 words.\n",
        "\n",
        "Review:\\n{prod_review}\n",
        "\n",
        "Feedback to delivery department:\\\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e1d6b4-e461-4dbb-9c13-3c6b4417f12d",
      "metadata": {
        "id": "b1e1d6b4-e461-4dbb-9c13-3c6b4417f12d"
      },
      "source": [
        "**3. Sentiment analysis:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb55d35d-3394-43ce-8c38-4d0e2bb05dd3",
      "metadata": {
        "id": "fb55d35d-3394-43ce-8c38-4d0e2bb05dd3"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\\\n",
        "Classify sentiment of the following product review.\n",
        "Give your answer as a single word, either \"positive\", \"negative\" or \"neutral\".\n",
        "\n",
        "Review:{prod_review}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0b4127-879b-4f6d-ace0-787d8ed1c185",
      "metadata": {
        "id": "ee0b4127-879b-4f6d-ace0-787d8ed1c185"
      },
      "source": [
        "**4. Entity Extraction:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46bd7741-f0d6-49f4-a987-dd14995f5288",
      "metadata": {
        "id": "46bd7741-f0d6-49f4-a987-dd14995f5288"
      },
      "outputs": [],
      "source": [
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the following entities from the text:\n",
        "\n",
        "Company name\n",
        "Date of contract\n",
        "Sum of contract\n",
        "Currency of a contract\n",
        "\n",
        "The review is delimited with triple backticks.\n",
        "Format your response as a JSON object with entities the keys and recoginized entities as values.\n",
        "If the information isn't present, use \"unknown\" as the value. Make your response as short as possible.\n",
        "\n",
        "Text: ```{lamp_review}```\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b269382-8b80-411b-83ed-3a435a39e34e",
      "metadata": {
        "id": "9b269382-8b80-411b-83ed-3a435a39e34e"
      },
      "source": [
        "**5. Topic recognition (open topics):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b8d691-ed63-4026-80b1-7a1569b68487",
      "metadata": {
        "id": "06b8d691-ed63-4026-80b1-7a1569b68487"
      },
      "outputs": [],
      "source": [
        "story = \"\"\"\n",
        "In a recent survey conducted by the government,\n",
        "public sector employees were asked to rate their level\n",
        "of satisfaction with the department they work at.\n",
        "The results revealed that NASA was the most popular\n",
        "department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings,\n",
        "stating, \"I'm not surprised that NASA came out on top.\n",
        "It's a great place to work with amazing people and\n",
        "incredible opportunities. I'm proud to be a part of\n",
        "such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team,\n",
        "with Director Tom Johnson stating, \"We are thrilled to\n",
        "hear that our employees are satisfied with their work at NASA.\n",
        "We have a talented and dedicated team who work tirelessly\n",
        "to achieve our goals, and it's fantastic to see that their\n",
        "hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the\n",
        "Social Security Administration had the lowest satisfaction\n",
        "rating, with only 45% of employees indicating they were\n",
        "satisfied with their job. The government has pledged to\n",
        "address the concerns raised by employees in the survey and\n",
        "work towards improving job satisfaction across all departments.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb4ad74-0520-4a6b-b05f-eb722d7fb223",
      "metadata": {
        "id": "feb4ad74-0520-4a6b-b05f-eb722d7fb223"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Determine five topics that are being discussed in the provided text.\n",
        "Make each item one or two words long.\n",
        "Give topics in Python list format.\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c187751a-b60b-45cd-b445-4eed98435c81",
      "metadata": {
        "id": "c187751a-b60b-45cd-b445-4eed98435c81"
      },
      "source": [
        "**6. Topic recognition (closed list of topics):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26cc6a67-b9b3-4718-8352-99746a8b6724",
      "metadata": {
        "id": "26cc6a67-b9b3-4718-8352-99746a8b6724"
      },
      "outputs": [],
      "source": [
        "topic_list = [\"NASA\", \"Security\", \"Biology\"]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Recognize a topic from the given topic list delimited in triple backticks\n",
        "that is being discussed in the text, which is delimited by triple quotes. If you cannot determine between these 3, return 'other'.\n",
        "\n",
        "Topic list: ```{topic_list}```\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\n",
        "Provide an answer only with one word, representing a determined topic.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d052637e-3947-437a-8caf-e517a856f288",
      "metadata": {
        "id": "d052637e-3947-437a-8caf-e517a856f288"
      },
      "source": [
        "**7. Translation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d70a74-a19b-4a7d-aceb-baf10c6ae1f5",
      "metadata": {
        "id": "d2d70a74-a19b-4a7d-aceb-baf10c6ae1f5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "text = \"Hello! How are you?\"\n",
        "system_prompt = \"Translate provided text to Spanish\"\n",
        "\n",
        "print(generate_response(prompt=text, system_prompt=system_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd71fdc-0388-4180-9cd9-a67c4d3ca96d",
      "metadata": {
        "id": "2cd71fdc-0388-4180-9cd9-a67c4d3ca96d"
      },
      "source": [
        "**8. Tone transforming:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad4c458f-5922-46eb-9c2f-f286e833937a",
      "metadata": {
        "id": "ad4c458f-5922-46eb-9c2f-f286e833937a"
      },
      "outputs": [],
      "source": [
        "text = \"Hi, man! Nice to see you. I will not be ready with my task, is it ok?\"\n",
        "prompt = f\"\"\" Translate the following from slang to a business letter: {text}. \"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7f54a7-b9c6-4872-8c1a-302afb03cfde",
      "metadata": {
        "id": "cc7f54a7-b9c6-4872-8c1a-302afb03cfde"
      },
      "source": [
        "**9. Spellcheck / Grammar check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38712a8d-2a61-4df5-8645-ee58caecb4f9",
      "metadata": {
        "id": "38712a8d-2a61-4df5-8645-ee58caecb4f9"
      },
      "outputs": [],
      "source": [
        "text = \"Hi, man! Nise to see you. I wont not be raedy with my task, it is ok?\"\n",
        "\n",
        "system_prompt = \"Proofread and correct user message. Explain made corrections.\"\n",
        "\n",
        "print(generate_response(prompt=text, system_prompt=system_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9f5353-4fe4-48e8-ae6f-d3820ad83455",
      "metadata": {
        "id": "4a9f5353-4fe4-48e8-ae6f-d3820ad83455"
      },
      "source": [
        "**10. Multiple tasks at the same time:**  \n",
        "Be aware that a prompt with multiple tasks tends to be less stable and correct than several prompts each for its own task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0d903f-4359-4c33-98a4-956d6732dbed",
      "metadata": {
        "id": "cf0d903f-4359-4c33-98a4-956d6732dbed"
      },
      "outputs": [],
      "source": [
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\" Identify the following items from the review text:\n",
        "\n",
        "Sentiment (positive or negative)\n",
        "Is the reviewer expressing anger? (true or false)\n",
        "Item purchased by reviewer\n",
        "Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks.\n",
        "Format your response as a JSON object with \"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\"as the value.\n",
        "Make your response as short as possible. Format the Anger value as a boolean.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc5ef29-7979-4d8b-9a13-70a7c0d71c6f",
      "metadata": {
        "id": "7bc5ef29-7979-4d8b-9a13-70a7c0d71c6f"
      },
      "source": [
        "These prompts could be a good baseline for your DS/ML task. You can consider them and add your own modifications, just make sure to avoid hallucinations and make them stable."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "toc-autonumbering": true
  },
  "nbformat": 4,
  "nbformat_minor": 5
}