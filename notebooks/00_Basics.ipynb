{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CherpanovNazim/learn-llm/blob/main/notebooks/00_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac8f20d",
      "metadata": {
        "id": "0ac8f20d"
      },
      "source": [
        "* [GitHub Repo](https://github.com/CherpanovNazim/learn-llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5af19ed4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5af19ed4",
        "outputId": "df222c22-ea32-4e70-91a7-9f989aab3681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.7 ms, sys: 2.86 ms, total: 20.6 ms\n",
            "Wall time: 3.92 s\n"
          ]
        }
      ],
      "source": [
        "# wait ~3 min for installations\n",
        "%%time\n",
        "\n",
        "!pip install -qU openai==1.40.3 vllm==0.5.4 transformers==4.44.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wait ~3 min for installations\n",
        "%%time\n",
        "\n",
        "# Load the default model\n",
        "DEFAULT_MODEL = {\"model\": \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\", \"api_base\": \"http://localhost:8000/v1\", \"api_key\": \"EMPTY\"}\n",
        "\n",
        "#run VLLM\n",
        "!nohup vllm serve {DEFAULT_MODEL['model']} --quantization awq --max-model-len=4096 > vllm.log &\n",
        "!tail -f vllm.log | grep -q \"Uvicorn running\" && echo \"Now you can start using the model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbYR367oX3i-",
        "outputId": "c5fe1036-eb67-444a-ae91-b38a1aae65e3"
      },
      "id": "LbYR367oX3i-",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n",
            "Now you can start using the model\n",
            "CPU times: user 569 ms, sys: 54.3 ms, total: 624 ms\n",
            "Wall time: 1min 37s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b064a83c",
      "metadata": {
        "id": "b064a83c"
      },
      "source": [
        "# Why LLAMA 3 ?\n",
        "\n",
        "* We mainly use GPT 3.5 or GPT 4 models for production use-cases\n",
        "* For this course we use LLAMA 3 8B model for educational purposes\n",
        "* LLAMA 3 is smaller, but still very powerful\n",
        "* We use OpenAI-compatible API for our LLAMA 3 - so this should would work for GPT 3.5 or GPT 4 as well"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Explainer class\n",
        "import json\n",
        "import openai\n",
        "from IPython.display import display, Markdown, Code, update_display\n",
        "import random\n",
        "from openai import OpenAI\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "class Explainer:\n",
        "    def __init__(self, system_prompt:str = None) -> None:\n",
        "        if system_prompt is None:\n",
        "            system_prompt = \"You are assistant for IT specialist who doesn't know NLP.\"\n",
        "        self.basic_system_prompt = system_prompt\n",
        "\n",
        "        self.config = DEFAULT_MODEL\n",
        "\n",
        "        self.client_explainer = OpenAI(base_url=self.config['api_base'], api_key = self.config['api_key'])\n",
        "\n",
        "    def llm_call(self, prompts, additional_system_prompt=\"\"):\n",
        "        system_prompt = self.basic_system_prompt + additional_system_prompt\n",
        "\n",
        "        completion = self.client_explainer.chat.completions.create(\n",
        "            model=self.config[\"model\"],\n",
        "            temperature=0,\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": prompts}])\n",
        "        print(completion)\n",
        "        return completion.choices[0].message.content.strip()\n",
        "\n",
        "    def stream_llm_call(self, prompts, additional_system_prompt=\"\"):\n",
        "        system_prompt = self.basic_system_prompt + additional_system_prompt\n",
        "\n",
        "        completion = self.client_explainer.chat.completions.create(\n",
        "            model=self.config[\"model\"],\n",
        "            temperature=0,\n",
        "            stream=True,\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": prompts}])\n",
        "\n",
        "        for chunk in completion:\n",
        "            if chunk.choices[0].delta.content is not None:\n",
        "                yield chunk.choices[0].delta.content\n",
        "\n",
        "    def __call__(self, text, output_type=\"markdown\"):\n",
        "        if output_type == \"markdown\":\n",
        "            data = list()\n",
        "            display_id = display(Markdown(''), display_id=True).display_id\n",
        "            for chunk in self.stream_llm_call(text, \"Format your answer using Markdown.\"):\n",
        "                data.append(chunk)\n",
        "                update_display(Markdown(''.join(data)), display_id=display_id)\n",
        "        elif output_type == \"code\":\n",
        "            results = self.llm_call(text, \"Provide only Python code without any side comments.\")\n",
        "            results = results.split(\"```\")[1].split(\"```\")[0]\n",
        "            display(Code(results))\n",
        "        else:\n",
        "            raise Exception(\"Unknown output type. Default: markdown, code\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sVYOdeh0ZQ_v"
      },
      "id": "sVYOdeh0ZQ_v",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43d7d094",
      "metadata": {
        "id": "43d7d094",
        "outputId": "70210203-2cdb-44d7-961b-0068c1fc490c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Comparison of LLMs and Other Deep Learning NLP Models**\n\n| **Model Type** | **Description** | **Key Characteristics** | **Primary Use Case** |\n| --- | --- | --- | --- |\n| **LLMs (Large Language Models)** | Pre-trained models that learn general language patterns and relationships | Auto-regressive architecture, self-supervised learning, massive parameter count | Conversational AI, text generation, language translation |\n| **Transformers** | Encoder-decoder architecture for sequence-to-sequence tasks | Self-attention mechanism, parallel processing, fixed input sequence length | Machine translation, text summarization, question answering |\n| **RNNs (Recurrent Neural Networks)** | Recurrent architecture for sequential data | Hidden state, recurrent connections, fixed sequence length | Speech recognition, sentiment analysis, language modeling |\n| **CNNs (Convolutional Neural Networks)** | Convolutional architecture for grid-like data | Local connections, shared weights, fixed spatial resolution | Text classification, sentiment analysis, topic modeling |\n| **BERT (Bidirectional Encoder Representations from Transformers)** | Pre-trained model for contextualized language understanding | Masked language modeling, next sentence prediction, contextualized embeddings | Question answering, sentiment analysis, text classification |\n\nNote: This table provides a high-level comparison of the key characteristics and primary use cases of each model type. LLMs are a specific type of model that encompasses the characteristics of other models, but they are not mutually exclusive."
          },
          "metadata": {}
        }
      ],
      "source": [
        "explain = Explainer()\n",
        "\n",
        "# use this class if you want to get some explanations\n",
        "# It uses LLAMA 3 to generate explanations :)\n",
        "explain(\"difference between LLMs and other deep learning NLP models in a small table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e0711f6f",
      "metadata": {
        "id": "e0711f6f"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Set the base URL and API key\n",
        "# For production apps it's preferable to use some secret management system and don't store the key in git repo :)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url = DEFAULT_MODEL['api_base'],\n",
        "    api_key = DEFAULT_MODEL['api_key']\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d19e0b-73ea-45b9-aa45-60e597704dca",
      "metadata": {
        "id": "d8d19e0b-73ea-45b9-aa45-60e597704dca"
      },
      "source": [
        "## Completion VS ChatCompletion API:  \n",
        "There is also 2 variants of API or how to use these models, for example in Python: `Completion` and `ChatCompletion`.  \n",
        "\n",
        "## [Completion API](https://platform.openai.com/docs/guides/gpt/completions-api):  \n",
        "For the most common case you need to specify:\n",
        "* model name\n",
        "* prompt that will have the task for the model and should be a string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0ba43208",
      "metadata": {
        "id": "0ba43208"
      },
      "outputs": [],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    n=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5b3c17ec-304b-4ddc-a6f3-4a09e02fe00b",
      "metadata": {
        "id": "5b3c17ec-304b-4ddc-a6f3-4a09e02fe00b",
        "outputId": "e62e210a-d983-4ae8-816e-99c75108500c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion(id='cmpl-4272ce3ec73a4b5394727eacf59b9056', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' Here are a few ideas to get you started:\\n* \"Cool treats for hot days\"\\n* \"Scoops of happiness\"\\n* \"The cream of the crop\"\\n* \"Chill out with us\"\\n* \"A scoop above the rest\"\\n* \"Sweet treats for sweet people\"\\n* \"The coolest place in town\"\\n* \"A taste of paradise\"\\n* \"Indulge in the sweet life\"\\n* \"A scoop of fun for everyone\"\\n* \"The perfect treat for any occasion\"\\n* \"Cool, creamy, and delicious\"\\n* \"A sweet escape from the ordinary\"\\n* \"The best ice cream in town, guaranteed\"\\n* \"Savor the flavor\"\\n* \"A scoop of joy\"\\n* \"The sweetest spot in town\"\\n* \"Cool treats for all seasons\"\\n* \"A taste of heaven on earth\"\\n* \"The ultimate ice cream experience\"\\n* \"A scoop above the rest, every time\"\\n* \"Sweet treats for every taste bud\"\\n* \"The coolest ice cream shop around\"\\n* \"A taste of pure bliss\"\\n* \"Cool, creamy, and utterly delicious\"\\n* \"The perfect treat for any time of day\"\\n* \"A scoop of happiness, every time\"\\n* \"The sweetest spot in town, guaranteed', stop_reason=None)], created=1723447439, model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=256, prompt_tokens=11, total_tokens=267))\n"
          ]
        }
      ],
      "source": [
        "# Example of an output as an completion object:\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8658708a-2bc1-4aab-a831-92b9a4d33abd",
      "metadata": {
        "id": "8658708a-2bc1-4aab-a831-92b9a4d33abd",
        "outputId": "d51ff50a-c9c4-4af4-a90b-144b46de74bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few ideas to get you started:\n",
            "* \"Cool treats for hot days\"\n",
            "* \"Scoops of happiness\"\n",
            "* \"The cream of the crop\"\n",
            "* \"Chill out with us\"\n",
            "* \"A scoop above the rest\"\n",
            "* \"Sweet treats for sweet people\"\n",
            "* \"The coolest place in town\"\n",
            "* \"A taste of paradise\"\n",
            "* \"Indulge in the sweet life\"\n",
            "* \"A scoop of fun for everyone\"\n",
            "* \"The perfect treat for any occasion\"\n",
            "* \"Cool, creamy, and delicious\"\n",
            "* \"A sweet escape from the ordinary\"\n",
            "* \"The best ice cream in town, guaranteed\"\n",
            "* \"Savor the flavor\"\n",
            "* \"A scoop of joy\"\n",
            "* \"The sweetest spot in town\"\n",
            "* \"Cool treats for all seasons\"\n",
            "* \"A taste of heaven on earth\"\n",
            "* \"The ultimate ice cream experience\"\n",
            "* \"A scoop above the rest, every time\"\n",
            "* \"Sweet treats for every taste bud\"\n",
            "* \"The coolest ice cream shop around\"\n",
            "* \"A taste of pure bliss\"\n",
            "* \"Cool, creamy, and utterly delicious\"\n",
            "* \"The perfect treat for any time of day\"\n",
            "* \"A scoop of happiness, every time\"\n",
            "* \"The sweetest spot in town, guaranteed\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e009b007-9b37-4deb-9605-91b4be5fee58",
      "metadata": {
        "id": "e009b007-9b37-4deb-9605-91b4be5fee58"
      },
      "source": [
        "Let's cover most important parameters for API:  \n",
        "- `temperature` -- aka randomness or 'creativity' of an output, `[0, 1]`. 0 -- no random, the output will be the same all the executions. 1 -- the output will be very different each execution.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b50c212a-0b03-4a0d-820f-ec9ae34258d7",
      "metadata": {
        "id": "b50c212a-0b03-4a0d-820f-ec9ae34258d7",
        "outputId": "61d63f65-9916-40fe-86dd-57a438c4ecbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few ideas to consider:\n",
            "\n",
            "A. \"Cool Treats for a Hot Day\"\n",
            "B. \"Creamy Concoctions to Savor\"\n",
            "C. \"Indulge in the Sweet Life\"\n",
            "D. \"A Scoop Above the Rest\"\n",
            "E. \"The Cream of the Crop, Always\"\n",
            "F. \"Chill Out with Us\"\n",
            "\n",
            "Which tagline do you think best captures the essence of an ice cream shop? Why?\n",
            "\n",
            "I think the best tagline is option A, \"Cool Treats for a Hot Day\". This tagline is catchy and straightforward, emphasizing the refreshing and cooling aspect of ice cream, especially during hot weather. It's a simple and effective way to convey the shop's product and appeal to customers looking for a sweet escape from the heat.\n",
            "\n",
            "The other options have their own merits, but they are not as effective as option A. Option B, \"Creamy Concoctions to Savor\", is a bit more generic and doesn't specifically convey the cooling aspect of ice cream. Option C, \"Indulge in the Sweet Life\", is a bit too vague and could apply to any sweet treat, not just ice cream. Option D, \"A Scoop Above the Rest\", is a play on words, but it's\n"
          ]
        }
      ],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=256,\n",
        "    n=1,\n",
        ")\n",
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066a3580-95ff-4c61-853e-c35a5295f76e",
      "metadata": {
        "id": "066a3580-95ff-4c61-853e-c35a5295f76e"
      },
      "source": [
        "* `max_tokens` -- limits number of tokens in the output. It works like too low values will shrink your output, but not making the model to generate very short output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bd5b020d-4b5b-4aef-951e-32147d3f49f0",
      "metadata": {
        "id": "bd5b020d-4b5b-4aef-951e-32147d3f49f0",
        "outputId": "2cf6af2c-e0e0-4321-ac23-548e7f570491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sweet treats indeed! Here\n"
          ]
        }
      ],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=5,\n",
        "    n=1,\n",
        ")\n",
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "060a6675-38e5-434b-8882-9b972f35a871",
      "metadata": {
        "id": "060a6675-38e5-434b-8882-9b972f35a871"
      },
      "source": [
        "* `n` -- how many output variants to generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4850b1ad-4e19-4066-8300-7ef34f320c20",
      "metadata": {
        "id": "4850b1ad-4e19-4066-8300-7ef34f320c20"
      },
      "outputs": [],
      "source": [
        "response = client.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    prompt=\"Write a tagline for an ice cream shop.\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=256,\n",
        "    n=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "94169ea9",
      "metadata": {
        "id": "94169ea9",
        "outputId": "72c75f5e-28bb-441f-d332-c83aa9aa8250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few ideas to get you started:\n",
            "\n",
            "* \"Cool treats, warm smiles\"\n",
            "* \"The sweetest spot in town\"\n",
            "* \"Chill out with a scoop (or two)\"\n",
            "* \"Savor the flavor\"\n",
            "* \"Freezing your worries away, one cone at a time\"\n",
            "* \"The best way to beat the heat\"\n",
            "* \"A scoop above the rest\"\n",
            "* \"Life's a sundae, and then you chill\"\n",
            "* \"A cool treat for a hot day\"\n",
            "* \"Indulge in the sweet stuff\"\n",
            "\n",
            "Which one do you think is the most effective tagline? Why?\n",
            "\n",
            "It depends on the target audience and the overall tone of the ice cream shop. For example, if the shop is targeting families with young children, a tagline like \"Cool treats, warm smiles\" might be the most effective, as it conveys a sense of fun and happiness. If the shop is targeting adults, a tagline like \"A scoop above the rest\" might be more appealing, as it suggests a high level of quality and sophistication.\n",
            "\n",
            "Overall, the most effective tagline will be one that is memorable, easy to understand, and resonates with the target audience. It should also give customers a sense of what the shop is about and\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "60ded41a",
      "metadata": {
        "id": "60ded41a",
        "outputId": "0e38a120-4998-4416-877f-adf82c209af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shop is called \"Chill Out\" and it's a healthy ice cream shop that uses organic ingredients and is free from common allergens like peanuts and tree nuts, dairy and egg whites. The tagline should reflect the unique aspects of this shop.\n",
            "Here are a few options:\n",
            "1. \"Cool Down, Without the Guilt\"\n",
            "2. \"Chill Out: Where Peace of Mind Meets Sweet Treats\"\n",
            "3. \"Indulge in the Calm\"\n",
            "4. \"Free from Fuss, Full of Flavor\"\n",
            "5. \"Allergy-Friendly Ice Cream for a Chill Life\"\n",
            "6. \"The Coolest Scoop on Healthy Ice Cream\"\n",
            "7. \"Unwind with a Conscience\"\n",
            "8. \"Purely Delicious, Purely Safe\"\n",
            "9. \"A Chill Atmosphere, and a Clear Conscience\"\n",
            "10. \"Sweet Treats, Safe for All\"\n",
            "\n",
            "Choose the one that best reflects the unique aspects of the shop, or feel free to modify any of these taglines to create the perfect fit.\n",
            "\n",
            "The best answer is: \"Allergy-Friendly Ice Cream for a Chill Life\"\n",
            "\n",
            "This tagline perfectly captures the unique aspects of the shop, highlighting its allergy-friendly approach while also emphasizing the relaxed and calming atmosphere of the shop.\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[1].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39b39b8-1b8f-4e01-acc4-f89d2bfcbdd4",
      "metadata": {
        "id": "e39b39b8-1b8f-4e01-acc4-f89d2bfcbdd4"
      },
      "source": [
        "Be aware, it's meaningless to use `n > 1` and `temperature == 0` since you will get `n` equal results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd35cf0-4221-4f8f-bc60-d1b4adaa1bfa",
      "metadata": {
        "id": "6fd35cf0-4221-4f8f-bc60-d1b4adaa1bfa"
      },
      "source": [
        "Other parameters you can check on official [OpenAI page](https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens:~:text=given%20chat%20conversation.-,Request%20body,-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5932a791-66c6-4170-a882-8a0f62466ebd",
      "metadata": {
        "id": "5932a791-66c6-4170-a882-8a0f62466ebd"
      },
      "source": [
        "## [ChatCompletion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api):  \n",
        "\n",
        "For the most common case you need to specify:\n",
        "* model name\n",
        "* messages in the form of list of dictionaries; they will emulate a chat between a user and an assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9c970684-b402-4c3f-a06a-54847feb19c0",
      "metadata": {
        "id": "9c970684-b402-4c3f-a06a-54847feb19c0"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=DEFAULT_MODEL[\"model\"],\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "35698270-1d09-4247-b790-2ea51dc506f1",
      "metadata": {
        "id": "35698270-1d09-4247-b790-2ea51dc506f1",
        "outputId": "90b9c86c-1506-4cd6-9436-47cff2f0c339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chat-b01b30343c2f4e0e90f8f1f3ec62488d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The 2020 World Series was played at Petco Park in San Diego, California, and Globe Life Field in Arlington, Texas, due to the coronavirus pandemic and the usual rotation of home-field advantage for the teams. However, the Los Angeles Dodgers played most of the series at Globe Life Field, and they also played some games at Dodger Stadium and San Diego's Petco Park.\", refusal=None, role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1723447645, model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=79, prompt_tokens=79, total_tokens=158))\n"
          ]
        }
      ],
      "source": [
        "# Example of an output completion object:\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7d9403ca-f718-4f3b-bf84-482e2c4fb81f",
      "metadata": {
        "id": "7d9403ca-f718-4f3b-bf84-482e2c4fb81f",
        "outputId": "1d9f3ca8-0832-4c92-ee18-b81843f7bdb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2020 World Series was played at Petco Park in San Diego, California, and Globe Life Field in Arlington, Texas, due to the coronavirus pandemic and the usual rotation of home-field advantage for the teams. However, the Los Angeles Dodgers played most of the series at Globe Life Field, and they also played some games at Dodger Stadium and San Diego's Petco Park.\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104baad5-b52f-4a55-8c38-339ebe8aea65",
      "metadata": {
        "id": "104baad5-b52f-4a55-8c38-339ebe8aea65"
      },
      "source": [
        "The **role** here is specifying who exactly sends the message, whether its from Human or from AI.  \n",
        "- **system role** is not required, but helpful to specify how a model should consider itself.  \n",
        "[From OpenAI documentation](https://platform.openai.com/docs/guides/gpt/chat-completions-api#:~:text=The%20system%20message,a%20helpful%20assistant.%22):  \n",
        "\"The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"  \n",
        "- **human role** are possible inputs from the user helping a model to 'fit' on them\n",
        "- **assistant** -- examples of a desired behavior on the user's inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732f09c1-4c98-4e40-8e5d-939e4727fa1d",
      "metadata": {
        "id": "732f09c1-4c98-4e40-8e5d-939e4727fa1d"
      },
      "source": [
        "# 📝 Prompt Engineering:  \n",
        "In this section we will cover:\n",
        "* Existing best techniques for prompt engineerging\n",
        "* Completion prompts for some Data Science tasks  \n",
        "\n",
        "\n",
        "*[References] This section is based on [DeepLearning.ai course](https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction?_gl=1*tjnub9*_ga*MzgyMDU1MDc3LjE2ODMwNTU2MzE.*_ga_PZF1GBS1R1*MTY4Nzk0NjM3My45LjEuMTY4Nzk0Nzk4Ni4zOC4wLjA.).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "38b8e7fb-57c5-439f-9397-6a8b31c84c4d",
      "metadata": {
        "id": "38b8e7fb-57c5-439f-9397-6a8b31c84c4d"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt: str, system_prompt: str = None):\n",
        "    if system_prompt is None:\n",
        "        system_prompt = \"Just follow user instructions and don't communicate like \\\"Sure!\\\" or \\\"I hope this helps\\\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=DEFAULT_MODEL[\"model\"],\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "              {\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3a359a-566e-4af0-ba66-219e36a77283",
      "metadata": {
        "id": "6a3a359a-566e-4af0-ba66-219e36a77283"
      },
      "source": [
        "## Best practices:  \n",
        "Now let's observe and try different techniques helping you to construct a comprehensive and effective prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85a877b-4474-4d6f-a3a9-b3f8d2d20304",
      "metadata": {
        "id": "a85a877b-4474-4d6f-a3a9-b3f8d2d20304"
      },
      "source": [
        "**1. Use delimiters to indicate distinct parts of the input clearly:**\n",
        "\n",
        "Delimiters can be anything like: ```, \"\"\", < >, \\<tag> \\</tag>. It could help model focusing on specific parts of the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "be353bde-5609-4d1f-907c-46c40835790f",
      "metadata": {
        "id": "be353bde-5609-4d1f-907c-46c40835790f",
        "outputId": "37026666-f998-4b0c-aacf-12f525a3ee89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jupiter is the largest planet in the Solar System, a gas giant known since ancient times and visible to the naked eye as one of the brightest objects in the night sky.\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "Jupiter is the fifth planet from the Sun and the largest in the Solar System.\n",
        "It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half\n",
        "times that of all the other planets in the Solar System combined.\n",
        "Jupiter is one of the brightest objects visible to the naked eye in the night sky,\n",
        "and has been known to ancient civilizations since before recorded history.\n",
        "It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be\n",
        "bright enough for its reflected light to cast visible shadows,[20] and is on average\n",
        "the third-brightest natural object in the night sky after the Moon and Venus.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Summarize the text delimited by triple backticks into a single sentence. ```{text}``` \"\"\"\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b42760-edf8-422a-9f91-c9ade8d2daca",
      "metadata": {
        "id": "e2b42760-edf8-422a-9f91-c9ade8d2daca"
      },
      "source": [
        "**2. Ask for a structured output:**\n",
        "\n",
        "You can as about JSON, HTML or any other reasonable format. JSON could be useful in many cases, let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "240f6033-c796-430f-a444-ff419ed16f0c",
      "metadata": {
        "id": "240f6033-c796-430f-a444-ff419ed16f0c",
        "outputId": "5b5f4985-62a8-4d8d-e5c9-46cdcbee44cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"books\": [\n",
            "    {\n",
            "      \"book_id\": 1,\n",
            "      \"title\": \"The Last Dreamer\",\n",
            "      \"author\": \"Ava Morales\",\n",
            "      \"genre\": \"Fantasy\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 2,\n",
            "      \"title\": \"The Memory Weaver\",\n",
            "      \"author\": \"Ethan Blackwood\",\n",
            "      \"genre\": \"Science Fiction\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 3,\n",
            "      \"title\": \"The Shadowlands\",\n",
            "      \"author\": \"Lila Grey\",\n",
            "      \"genre\": \"Horror\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three made-up book titles along\n",
        "with their authors and genres. Provide them in JSON\n",
        "format with the following keys: book_id, title, author, genre.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d4c9fa-6d5a-4705-a32d-f319a40646de",
      "metadata": {
        "id": "28d4c9fa-6d5a-4705-a32d-f319a40646de"
      },
      "source": [
        "**3. Ask the model to check whether conditions are satisfied:**  \n",
        "It's like `IF-ELSE` statement inside a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b04ccb70-b075-4290-b5fb-e63568e6f68b",
      "metadata": {
        "id": "b04ccb70-b075-4290-b5fb-e63568e6f68b",
        "outputId": "aab62d8d-7765-4589-9601-eae480444afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps provided:\n",
            " Step 1 - Get some water boiling.\n",
            "Step 2 - Grab a cup and put a tea bag in it.\n",
            "Step 3 - Once the water is hot enough, pour it over the tea bag.\n",
            "Step 4 - Let it sit for a bit so the tea can steep.\n",
            "Step 5 - After a few minutes, take out the tea bag.\n",
            "Step 6 - If you like, add some sugar or milk to taste.\n",
            "Step 7 - Enjoy your delicious cup of tea.\n",
            "\n",
            " ------------------------------ \n",
            "\n",
            "Steps missed:\n",
            " No steps provided.\n"
          ]
        }
      ],
      "source": [
        "recipe_w_steps = f\"\"\"\n",
        "Making a cup of tea is easy! First, you need to get some\n",
        " water boiling. While that's happening,\n",
        "grab a cup and put a tea bag in it. Once the water is\n",
        "hot enough, just pour it over the tea bag.\n",
        "Let it sit for a bit so the tea can steep. After a\n",
        "few minutes, take out the tea bag. If you\n",
        "like, you can add some sugar or milk to taste.\n",
        "And that's it! You've got yourself a delicious\n",
        "cup of tea to enjoy.\n",
        "\"\"\"\n",
        "\n",
        "recipe_wo_steps = f\"\"\"\n",
        "Making a cup of tea is easy! Just do it!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = \"\"\"\n",
        "You will be provided with text delimited by triple quotes. If it contains some instructions, re-write those instructions in the following format:\n",
        "Step 1 - ... Step 2 - ... Step N - ...\n",
        "\n",
        "If provided text does not contain any instructions, simply write \\\"No steps provided.\\\".\n",
        "\n",
        "Text: \\\"\\\"\\\"{recipe}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "print(\"Steps provided:\\n\", generate_response(prompt=prompt.format(recipe=recipe_w_steps)))\n",
        "print(\"\\n\", \"-\"*30, \"\\n\")\n",
        "print(\"Steps missed:\\n\", generate_response(prompt=prompt.format(recipe=recipe_wo_steps)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37bf1a7-8869-45af-aaa2-a2897a17939c",
      "metadata": {
        "id": "e37bf1a7-8869-45af-aaa2-a2897a17939c"
      },
      "source": [
        "**4. Focus on specific aspects:**  \n",
        "Specifically ask a model to pay more attention on some details according to the task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "90df4576-bdc8-47a2-ae07-c10ead40b45d",
      "metadata": {
        "tags": [],
        "id": "90df4576-bdc8-47a2-ae07-c10ead40b45d",
        "outputId": "8eecc6ee-3513-4916-96cb-eab20ad3c620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This stylish office chair is crafted from high-quality materials, featuring a 10mm cast aluminum shell with a durable PA6/PA66 coating, paired with a HD36 foam seat and pneumatic adjust for comfortable use. Available in various configurations to suit home or business settings.\n"
          ]
        }
      ],
      "source": [
        "fact_sheet_chair = \"\"\"\n",
        "OVERVIEW\n",
        "- Part of a beautiful family of mid-century inspired office furniture,\n",
        "including filing cabinets, desks, bookcases, meeting tables, and more.\n",
        "- Several options of shell color and base finishes.\n",
        "- Available with plastic back and front upholstery (SWC-100)\n",
        "or full upholstery (SWC-110) in 10 fabric and 6 leather options.\n",
        "- Base finish options are: stainless steel, matte black,\n",
        "gloss white, or chrome.\n",
        "- Chair is available with or without armrests.\n",
        "- Suitable for home or business settings.\n",
        "- Qualified for contract use.\n",
        "\n",
        "CONSTRUCTION\n",
        "- 5-wheel plastic coated aluminum base.\n",
        "- Pneumatic chair adjust for easy raise/lower action.\n",
        "\n",
        "DIMENSIONS\n",
        "- WIDTH 53 CM | 20.87”\n",
        "- DEPTH 51 CM | 20.08”\n",
        "- HEIGHT 80 CM | 31.50”\n",
        "- SEAT HEIGHT 44 CM | 17.32”\n",
        "- SEAT DEPTH 41 CM | 16.14”\n",
        "\n",
        "OPTIONS\n",
        "- Soft or hard-floor caster options.\n",
        "- Two choices of seat foam densities:\n",
        " medium (1.8 lb/ft3) or high (2.8 lb/ft3)\n",
        "- Armless or 8 position PU armrests\n",
        "\n",
        "MATERIALS\n",
        "SHELL BASE GLIDER\n",
        "- Cast Aluminum with modified nylon PA6/PA66 coating.\n",
        "- Shell thickness: 10 mm.\n",
        "SEAT\n",
        "- HD36 foam\n",
        "\n",
        "COUNTRY OF ORIGIN\n",
        "- Italy\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\n",
        "Write a product description based on the information provided in the technical specifications delimited by triple backticks.\n",
        "\n",
        "The description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.\n",
        "\n",
        "Use at most 50 words.\n",
        "Technical specifications: ```{fact_sheet_chair}```\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b93c94-6075-47b1-83dc-fe8b0d8c26cb",
      "metadata": {
        "id": "28b93c94-6075-47b1-83dc-fe8b0d8c26cb"
      },
      "source": [
        "**5. Specify the model’s role:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "c95ca766-462b-413a-a8e6-0d1e7f91110f",
      "metadata": {
        "id": "c95ca766-462b-413a-a8e6-0d1e7f91110f",
        "outputId": "134cef0e-56b8-4bcc-bf00-c4640bf50c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Pizza Palace! I'll be happy to take your order today. What can I get for you?\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "You are OrderBot - an automated service to collect orders for a pizza restaurant.\n",
        "You first greet the customer, then collects the order, and then asks if it's a pickup or delivery.\n",
        "You wait to collect the entire order, then summarize it and check for a final time if the customer wants to add anything else.\n",
        "\"\"\"\n",
        "\n",
        "# Please note that we provided this prompt to system prompt argument\n",
        "print(generate_response(prompt='Hi!', system_prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35dec9e3-089a-437f-91d2-50ab80475ef5",
      "metadata": {
        "id": "35dec9e3-089a-437f-91d2-50ab80475ef5"
      },
      "source": [
        "## Prompt examples for Data Science / Machine Learning tasks:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "0f847ed1-fecd-430d-a7bd-4482bf6002ba",
      "metadata": {
        "id": "0f847ed1-fecd-430d-a7bd-4482bf6002ba"
      },
      "outputs": [],
      "source": [
        "prod_review = \"\"\"\\\n",
        "Got this panda plush toy for my daughter's birthday, \\\n",
        "who loves it and takes it everywhere. It's soft and \\\n",
        "super cute, and its face has a friendly look. It's \\\n",
        "a bit small for what I paid though. I think there \\\n",
        "might be other options that are bigger for the \\\n",
        "same price. It arrived a day earlier than expected, \\\n",
        "so I got to play with it myself before I gave it \\\n",
        "to her.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a7cade3-e51f-4033-9156-bfd473c15dd9",
      "metadata": {
        "id": "0a7cade3-e51f-4033-9156-bfd473c15dd9"
      },
      "source": [
        "**1. Summarization with focus:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5fc02f0a-91b1-4239-94db-98a281ab52d7",
      "metadata": {
        "id": "5fc02f0a-91b1-4239-94db-98a281ab52d7",
        "outputId": "55ece4ec-82c4-49cf-9301-c3963ce90034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer felt the plush toy was small for its price, suggesting a potential value perception issue, and may consider larger options at the same price.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a short summary of a product review from an ecommerce site \\\n",
        "to give feedback to the pricing deparmtment, responsible for determining the price of the product.\n",
        "Summarize the review below in at most 30 words, and focusing on any aspects that are relevant to the price and perceived value.\n",
        "\n",
        "Review:\\n{prod_review}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61aeb86c-104f-4695-bc4b-b52a05684a07",
      "metadata": {
        "id": "61aeb86c-104f-4695-bc4b-b52a05684a07"
      },
      "source": [
        "**2. Information extraction:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "de89fd7b-5098-4687-877b-90c6c430cc73",
      "metadata": {
        "id": "de89fd7b-5098-4687-877b-90c6c430cc73",
        "outputId": "8e698cf7-59e7-4a4a-96dd-5859e622e635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delivery arrived a day earlier than expected, which was a pleasant surprise. However, the customer felt the product was a bit small for the price paid.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize the review below in order to give feedback to delivery department. Limit to 30 words.\n",
        "\n",
        "Review:\\n{prod_review}\n",
        "\n",
        "Feedback to delivery department:\\\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e1d6b4-e461-4dbb-9c13-3c6b4417f12d",
      "metadata": {
        "id": "b1e1d6b4-e461-4dbb-9c13-3c6b4417f12d"
      },
      "source": [
        "**3. Sentiment analysis:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "fb55d35d-3394-43ce-8c38-4d0e2bb05dd3",
      "metadata": {
        "id": "fb55d35d-3394-43ce-8c38-4d0e2bb05dd3",
        "outputId": "dfb437bf-4c05-4579-8157-92f6addac26c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\\\n",
        "Classify sentiment of the following product review.\n",
        "Give your answer as a single word, either \"positive\", \"negative\" or \"neutral\".\n",
        "\n",
        "Review:{prod_review}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0b4127-879b-4f6d-ace0-787d8ed1c185",
      "metadata": {
        "id": "ee0b4127-879b-4f6d-ace0-787d8ed1c185"
      },
      "source": [
        "**4. Entity Extraction:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "46bd7741-f0d6-49f4-a987-dd14995f5288",
      "metadata": {
        "id": "46bd7741-f0d6-49f4-a987-dd14995f5288",
        "outputId": "c057a487-ebc8-41ff-da88-0c2ef92ab50f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Company name\": \"Lumina\",\n",
            "  \"Date of contract\": \"unknown\",\n",
            "  \"Sum of contract\": \"unknown\",\n",
            "  \"Currency of a contract\": \"unknown\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the following entities from the text:\n",
        "\n",
        "Company name\n",
        "Date of contract\n",
        "Sum of contract\n",
        "Currency of a contract\n",
        "\n",
        "The review is delimited with triple backticks.\n",
        "Format your response as a JSON object with entities the keys and recoginized entities as values.\n",
        "If the information isn't present, use \"unknown\" as the value. Make your response as short as possible.\n",
        "\n",
        "Text: ```{lamp_review}```\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b269382-8b80-411b-83ed-3a435a39e34e",
      "metadata": {
        "id": "9b269382-8b80-411b-83ed-3a435a39e34e"
      },
      "source": [
        "**5. Topic recognition (open topics):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "06b8d691-ed63-4026-80b1-7a1569b68487",
      "metadata": {
        "id": "06b8d691-ed63-4026-80b1-7a1569b68487"
      },
      "outputs": [],
      "source": [
        "story = \"\"\"\n",
        "In a recent survey conducted by the government,\n",
        "public sector employees were asked to rate their level\n",
        "of satisfaction with the department they work at.\n",
        "The results revealed that NASA was the most popular\n",
        "department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings,\n",
        "stating, \"I'm not surprised that NASA came out on top.\n",
        "It's a great place to work with amazing people and\n",
        "incredible opportunities. I'm proud to be a part of\n",
        "such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team,\n",
        "with Director Tom Johnson stating, \"We are thrilled to\n",
        "hear that our employees are satisfied with their work at NASA.\n",
        "We have a talented and dedicated team who work tirelessly\n",
        "to achieve our goals, and it's fantastic to see that their\n",
        "hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the\n",
        "Social Security Administration had the lowest satisfaction\n",
        "rating, with only 45% of employees indicating they were\n",
        "satisfied with their job. The government has pledged to\n",
        "address the concerns raised by employees in the survey and\n",
        "work towards improving job satisfaction across all departments.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "feb4ad74-0520-4a6b-b05f-eb722d7fb223",
      "metadata": {
        "id": "feb4ad74-0520-4a6b-b05f-eb722d7fb223",
        "outputId": "8b42fa5c-c581-4059-cb1e-5fc6dcdef940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NASA', 'Government', 'Job Satisfaction', 'Employee', 'Survey']\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Determine five topics that are being discussed in the provided text.\n",
        "Make each item one or two words long.\n",
        "Give topics in Python list format.\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c187751a-b60b-45cd-b445-4eed98435c81",
      "metadata": {
        "id": "c187751a-b60b-45cd-b445-4eed98435c81"
      },
      "source": [
        "**6. Topic recognition (closed list of topics):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "26cc6a67-b9b3-4718-8352-99746a8b6724",
      "metadata": {
        "id": "26cc6a67-b9b3-4718-8352-99746a8b6724",
        "outputId": "392d665d-ceae-411b-9f86-82743a00972d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NASA\n"
          ]
        }
      ],
      "source": [
        "topic_list = [\"NASA\", \"Security\", \"Biology\"]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Recognize a topic from the given topic list delimited in triple backticks\n",
        "that is being discussed in the text, which is delimited by triple quotes. If you cannot determine between these 3, return 'other'.\n",
        "\n",
        "Topic list: ```{topic_list}```\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\n",
        "Provide an answer only with one word, representing a determined topic.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d052637e-3947-437a-8caf-e517a856f288",
      "metadata": {
        "id": "d052637e-3947-437a-8caf-e517a856f288"
      },
      "source": [
        "**7. Translation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d2d70a74-a19b-4a7d-aceb-baf10c6ae1f5",
      "metadata": {
        "tags": [],
        "id": "d2d70a74-a19b-4a7d-aceb-baf10c6ae1f5",
        "outputId": "8f17467b-4c10-4b2c-9a88-776fde43e248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola! ¿Cómo estás?\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello! How are you?\"\n",
        "system_prompt = \"Translate provided text to Spanish\"\n",
        "\n",
        "print(generate_response(prompt=text, system_prompt=system_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd71fdc-0388-4180-9cd9-a67c4d3ca96d",
      "metadata": {
        "id": "2cd71fdc-0388-4180-9cd9-a67c4d3ca96d"
      },
      "source": [
        "**8. Tone transforming:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ad4c458f-5922-46eb-9c2f-f286e833937a",
      "metadata": {
        "id": "ad4c458f-5922-46eb-9c2f-f286e833937a",
        "outputId": "4c92b0b7-c962-4002-edb4-edeff7f0a236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear [Recipient],\n",
            "\n",
            "I am pleased to see you. I wanted to inform you that I will not be able to complete my task on time. I was wondering if this would be acceptable.\n",
            "\n",
            "Sincerely,\n",
            "[Your Name]\n"
          ]
        }
      ],
      "source": [
        "text = \"Hi, man! Nice to see you. I will not be ready with my task, is it ok?\"\n",
        "prompt = f\"\"\" Translate the following from slang to a business letter: {text}. \"\"\"\n",
        "\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7f54a7-b9c6-4872-8c1a-302afb03cfde",
      "metadata": {
        "id": "cc7f54a7-b9c6-4872-8c1a-302afb03cfde"
      },
      "source": [
        "**9. Spellcheck / Grammar check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "38712a8d-2a61-4df5-8645-ee58caecb4f9",
      "metadata": {
        "id": "38712a8d-2a61-4df5-8645-ee58caecb4f9",
        "outputId": "f6649d34-e6f1-4337-d918-5ed54d0dce0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected message:\n",
            "\"Hi, man! Nice to see you. I won't be ready with my task, it's okay?\"\n",
            "\n",
            "Corrected corrections:\n",
            "1. 'Nise' -> 'Nice': 'Nise' is not a word in English, whereas 'Nice' is an adjective meaning pleasant or agreeable.\n",
            "2. 'wont' -> 'won't': 'Wont' is a verb meaning to be accustomed to, whereas 'won't' is a contraction of 'will not', meaning to refuse or decline.\n",
            "3. 'raedy' -> 'ready': 'Raedy' is not a word in English, whereas 'ready' is an adjective meaning prepared or equipped.\n",
            "4. 'wont not' -> 'won't be': 'Wont not' is incorrect, whereas 'won't be' is the correct phrase to use in this context.\n",
            "5. 'it is ok?' -> 'it's okay?': 'It is' is a phrase used to make a statement, whereas 'it's' is a contraction of 'it is', and 'okay' is an informal way of saying 'all right' or 'acceptable'.\n"
          ]
        }
      ],
      "source": [
        "text = \"Hi, man! Nise to see you. I wont not be raedy with my task, it is ok?\"\n",
        "\n",
        "system_prompt = \"Proofread and correct user message. Explain made corrections.\"\n",
        "\n",
        "print(generate_response(prompt=text, system_prompt=system_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9f5353-4fe4-48e8-ae6f-d3820ad83455",
      "metadata": {
        "id": "4a9f5353-4fe4-48e8-ae6f-d3820ad83455"
      },
      "source": [
        "**10. Multiple tasks at the same time:**  \n",
        "Be aware that a prompt with multiple tasks tends to be less stable and correct than several prompts each for its own task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "cf0d903f-4359-4c33-98a4-956d6732dbed",
      "metadata": {
        "id": "cf0d903f-4359-4c33-98a4-956d6732dbed",
        "outputId": "6a592ccc-c1c6-4451-9d75-cdbfc28c2160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Sentiment\": \"positive\",\n",
            "  \"Anger\": false,\n",
            "  \"Item\": \"lamp\",\n",
            "  \"Brand\": \"Lumina\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\" Identify the following items from the review text:\n",
        "\n",
        "Sentiment (positive or negative)\n",
        "Is the reviewer expressing anger? (true or false)\n",
        "Item purchased by reviewer\n",
        "Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks.\n",
        "Format your response as a JSON object with \"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\"as the value.\n",
        "Make your response as short as possible. Format the Anger value as a boolean.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "print(generate_response(prompt=prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc5ef29-7979-4d8b-9a13-70a7c0d71c6f",
      "metadata": {
        "id": "7bc5ef29-7979-4d8b-9a13-70a7c0d71c6f"
      },
      "source": [
        "These prompts could be a good baseline for your DS/ML task. You can consider them and add your own modifications, just make sure to avoid hallucinations and make them stable."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "toc-autonumbering": true,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}