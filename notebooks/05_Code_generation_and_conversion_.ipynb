{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CherpanovNazim/learn-llm/blob/main/notebooks/05_Code_generation_and_conversion_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtIsyi3EVps-"
      },
      "source": [
        "# Code generation and conversion\n",
        "In this notebook, we'll solve some tasks that are related to code generation:\n",
        "\n",
        "* Creating a python function according to description.\n",
        "* Creating a function with additional constraints.\n",
        "* Creating a function, from inputs-outputs example.\n",
        "* Code convertion to optimize a function.\n",
        "* Infilling the missing part of a function.\n",
        "\n",
        "All tasks are solved using following steps:\n",
        "- Generate initial implementation of a function.\n",
        "- Generate unit tests using hints to steer the generation.\n",
        "- Iteratively prompt LLM to fix the issues that appear when running unit tests.\n",
        "\n",
        "Be carefull!\n",
        "Running the code that was generated by LLM can be dangerous!\n",
        "To minimize security risks, make sure you run it in an isolated sandbox environment!\n",
        "\n",
        "<hr style=\"border-top: 1px solid rgb(230, 230, 230);\">\n",
        "\n",
        "* [GitHub Repo](https://github.com/CherpanovNazim/learn-llm)\n",
        "\n",
        "<hr style=\"border-top: 1px solid rgb(230, 230, 230);\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZGZ3TGPVptB"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai==1.35.14 langchain==0.2.13 pandas==2.2.2 vllm==0.5.4 transformers==4.44.0 langchain-community==0.2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmptcpsIVptC"
      },
      "outputs": [],
      "source": [
        "# wait ~5 min for installations\n",
        "%%time\n",
        "\n",
        "import json\n",
        "import random\n",
        "import unittest\n",
        "\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "!wget -q 'https://raw.githubusercontent.com/CherpanovNazim/learn-llm/main/notebooks/utils/llm_for_code.py' -O llm_for_code.py\n",
        "!python3 llm_for_code.py\n",
        "\n",
        "from llm_for_code import (\n",
        "    contains_class,\n",
        "    contains_function,\n",
        "    extract_code,\n",
        "    run_unit_tests,\n",
        "    wrap_code,\n",
        ")\n",
        "\n",
        "#download widgets from GitHub\n",
        "!wget -q 'https://raw.githubusercontent.com/CherpanovNazim/learn-llm/main/notebooks/utils/widgets.py' -O widgets.py\n",
        "!python3 widgets.py\n",
        "\n",
        "from widgets import LLMCallVisualiser\n",
        "\n",
        "\n",
        "# Load the default model\n",
        "DEFAULT_MODEL = {\"model\": \"PrunaAI/ibm-granite-granite-8b-code-instruct-AWQ-4bit-smashed\", \"api_base\": \"http://localhost:8000/v1\", \"api_key\": \"EMPTY\"}\n",
        "\n",
        "#run VLLM\n",
        "!nohup vllm serve {DEFAULT_MODEL['model']} --quantization awq --max-model-len=4096 > vllm.log &\n",
        "!tail -f vllm.log | grep -q \"Uvicorn running\" && echo \"Now you can start using the model\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the base URL and API key.\n",
        "# For production apps it's preferable to use some secret management system and don't store the key in git repo :)\n",
        "client = openai.OpenAI(base_url=DEFAULT_MODEL[\"api_base\"], api_key=DEFAULT_MODEL[\"api_key\"])\n",
        "\n",
        "def chat_completion(\n",
        "    prompt,\n",
        "    system_prompt: str = None,\n",
        "    temperature: float = 0.,\n",
        "    max_tokens: int = None,\n",
        "    llm_call_visualiser: LLMCallVisualiser = None,\n",
        "    **kwargs\n",
        ") -> str:\n",
        "    if system_prompt is None:\n",
        "        # Here we define the output format\n",
        "        system_prompt = \"Generate some reasoning and then the python\"\\\n",
        "                        \" code wrapped into ```python\\n...\\n``` block.\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL[\"model\"],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        frequency_penalty=1,\n",
        "        timeout=120,\n",
        "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                  {\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True,  # To see generation in real-time\n",
        "        **kwargs)\n",
        "\n",
        "    try:\n",
        "        completion = \"\"\n",
        "        for chunk in response:\n",
        "            chunk_text = chunk.choices[0].delta.content\n",
        "            if chunk_text:\n",
        "                completion += chunk_text\n",
        "\n",
        "                # Update llm_call_visualiser UI element with next\n",
        "                #   chunk of streamed LLM output.\n",
        "                if llm_call_visualiser is not None:\n",
        "                    llm_call_visualiser.append_streamed_text(chunk_text)\n",
        "    finally:\n",
        "        if llm_call_visualiser is not None:\n",
        "            llm_call_visualiser.finalize_streamed_text()\n",
        "\n",
        "    return completion.strip(\" \\n\")"
      ],
      "metadata": {
        "id": "eLicDTLfW4G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8AZ1QQWVptC"
      },
      "source": [
        "# Defining code generation tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO-L9cZFVptD"
      },
      "outputs": [],
      "source": [
        "code_generation_tasks_examples = [\n",
        "    # Example 0:\n",
        "    # Creating a python function according to description.\n",
        "    {\n",
        "        # Description contains example of how versions look like,\n",
        "        #   rather than describing the version composition rules.\n",
        "        \"function_generation_instruction\":\n",
        "            \"Write version_compare function.\"\n",
        "            \" It must be able to compare versions like:\"\n",
        "            \" version_compare('1.2.3', '1.2.13.7').\",\n",
        "\n",
        "        # Less capable models might miss some important types of checks.\n",
        "        # We can explicitely ask which test cases to include if LLM can't come up with those itself.\n",
        "        \"unit_testing_hints\":\n",
        "            \"Include tests with different version parts lengths comparison:\"\n",
        "            \" version_compare('1.2.3', '1.2.13.7').\",\n",
        "    },\n",
        "\n",
        "    # Example 1:\n",
        "    # Creating a function, constraining not to wrap well-known implementation.\n",
        "    {\n",
        "        # If we want the full function code that is not simply a wraper for the\n",
        "        #   base64 implementation, then we can add such a constraint.\n",
        "        \"function_generation_instruction\":\n",
        "            \"Write str_to_base64 function. It must not use base64 lib.\",\n",
        "\n",
        "        # When generating unit tests, LLM might hallucinate the base64 encoded\n",
        "        #   expected values. So we ask LLM to compare to the values that are\n",
        "        #   being produced by the base64.b64encode function.\n",
        "        # Set the hint to None to see if LLM can generate robust expected values without it.\n",
        "        \"unit_testing_hints\":\n",
        "            \"The last line of each test case MUST be exactly (up to a single character):\"\n",
        "            \" self.assertEqual(str_to_base64(test_str), base64.b64encode(test_str.encode()).decode())\",\n",
        "    },\n",
        "\n",
        "    # Example 2:\n",
        "    # Creating a function, from inputs-outputs example.\n",
        "    {\n",
        "        # Sometimes we might need LLM to propose the implementation, when\n",
        "        #   only knowing the inputs-outputs of some function.\n",
        "        \"function_generation_instruction\":\n",
        "            \"Write function implementation and name it appropriately. When inputs are invalid, raise exception.\\n\"\n",
        "            \"\\n\"\n",
        "            \"def name_this_fn_appropriately(a: list[list[float]],\"\n",
        "            \" b: list[list[float]]) -> list[list[float]]\\n\"\n",
        "            \"Input : a = [[1, 7, 3],\\n\"\n",
        "            \"             [3, 5, 6],\\n\"\n",
        "            \"             [6, 8, 9]]\\n\"\n",
        "            \"        b = [[1, 1, 1, 2],\\n\"\n",
        "            \"            [6, 7, 3, 0],\\n\"\n",
        "            \"            [4, 5, 9, 1]]\\n\"\n",
        "            \"Output : [55, 65, 49, 5]\\n\"\n",
        "            \"         [57, 68, 72, 12]\\n\"\n",
        "            \"         [90, 107, 111, 21]\",\n",
        "\n",
        "        # When generating unit tests, LLM might generate incorrect\n",
        "        #   expected values. So we hope to reduce those by recommending\n",
        "        #   using numpy.\n",
        "        \"unit_testing_hints\":\n",
        "            \"Use numpy to generate expected values in test cases.\",\n",
        "    },\n",
        "\n",
        "    # Example 3:\n",
        "    # Code conversion.\n",
        "    {\n",
        "        # Here we ask to vectorize the function.\n",
        "        \"function_generation_instruction\":\n",
        "            \"Add vectorized function variant, so that function inputs, calculations\"\n",
        "            \" and outputs are all vectorized. Name new function as vectorized_calculate,\"\n",
        "            \" and output BOTH nonvectorized_calculate and vectorized_calculate functions:\\n\"\n",
        "            \"\\n\"\n",
        "            \"def nonvectorized_calculate(a, b):\\n\"\n",
        "            \"    if not a or not b:\\n\"\n",
        "            \"        return []\\n\"\n",
        "            \"\\n\"\n",
        "            \"    if len(a[0]) != len(b):\\n\"\n",
        "            \"        raise ValueError('Number of columns in the first matrix must be equal\"\n",
        "            \" to the number of rows in the second matrix')\\n\"\n",
        "            \"\\n\"\n",
        "            \"    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\\n\"\n",
        "            \"\\n\"\n",
        "            \"    for i in range(len(a)):\\n\"\n",
        "            \"        for j in range(len(b[0])):\\n\"\n",
        "            \"            for k in range(len(b)):\\n\"\n",
        "            \"                result[i][j] += a[i][k] * b[k][j]\\n\"\n",
        "            \"\\n\"\n",
        "            \"    for i in range(len(result)):\\n\"\n",
        "            \"        for j in range(len(result[0])):\\n\"\n",
        "            \"            result[i][j] = result[i][j] * result[i][j]\\n\"\n",
        "            \"\\n\"\n",
        "            \"    return result\\n\",\n",
        "\n",
        "        # We guide the model how to generate expected values.\n",
        "        # Set the hint to None to see if LLM can generate robust expected values without it.\n",
        "        \"unit_testing_hints\":\n",
        "            \"Function to test is vectorized_calculate. Generate expected values for\"\n",
        "            \" testing it using nonvectorized_calculate function and NEVER change its code.\",\n",
        "    },\n",
        "\n",
        "    # Example 4:\n",
        "    # Infilling the missing part of a function.\n",
        "    {\n",
        "        # Sometimes we need to reconstruct only a part of the code.\n",
        "        # But this is a relatively hard task for less capable LLMs which might\n",
        "        #   change the code out of the MISSING PART.\n",
        "        \"function_generation_instruction\":\n",
        "            \"Generate the MISSING part, but output the whole function\"\n",
        "            \" keeping EXACT COPY of the rest of the code.\\n\"\n",
        "            \"\\n\"\n",
        "            \"def compare_if_nested_lists_are_equal(l1: list, l2: list) -> bool:\\n\"\n",
        "            \"    \\\"\\\"\\\"\\n\"\n",
        "            \"    Compares nested lists, which can have any depth (e.g. \\n\"\n",
        "            \"      list[list[list[float]]]) and arbitrary lengths of lists\\n\"\n",
        "            \"    \\\"\\\"\\\"\\n\"\n",
        "            \"    \\n\"\n",
        "            \"    result = None\\n\"\n",
        "            \"    \\n\"\n",
        "            \"    # MISSING PART\\n\"\n",
        "            \"    \\n\"\n",
        "            \"            else:\\n\"\n",
        "            \"                if l1[i] != l2[i]:\\n\"\n",
        "            \"                    result = False\\n\"\n",
        "            \"                    break\\n\"\n",
        "            \"    \\n\"\n",
        "            \"    return result\\n\",\n",
        "\n",
        "        # We restrict the model not to generate too deep nested lists as expected values.\n",
        "        # Otherwise long chains might be wrong. For instance,\n",
        "        #   \"[1, [2, [3, [4, [5, [6, [7, [8, [9, [10]]]]]]]]]\" misses 1 closing \"]\".\n",
        "        \"unit_testing_hints\":\n",
        "            \"Use maximum depth of 4 for nested lists in test cases.\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCs5xd7vVptD"
      },
      "source": [
        "# Select code generation task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgNWELqPVptD"
      },
      "outputs": [],
      "source": [
        "# Here you can choose the example with the function generation task definition\n",
        "#   from the code_generation_tasks_examples\n",
        "#\n",
        "task_example_id = 0  # Change this to select a different task\n",
        "#\n",
        "print(f\"Selected code generation task example id:\\n{'-' * 64}\\n{task_example_id}\\n\\n\")\n",
        "\n",
        "code_generation_task = code_generation_tasks_examples[task_example_id]\n",
        "\n",
        "function_generation_instruction = code_generation_task[\"function_generation_instruction\"]\n",
        "print(f\"Function generation instruction:\\n{'-' * 64}\\n{function_generation_instruction}\\n\\n\")\n",
        "\n",
        "unit_testing_hints = code_generation_task[\"unit_testing_hints\"]\n",
        "print(f\"Function unit testing hints:\\n{'-' * 64}\\n{unit_testing_hints}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCsVE6j1VptE"
      },
      "source": [
        "# Initial code generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSU8p4J3VptE"
      },
      "outputs": [],
      "source": [
        "def generate_code(\n",
        "    prompt: str,\n",
        ") -> (str, str):\n",
        "    \"\"\"Generating code according to prompt\"\"\"\n",
        "\n",
        "    # Output UI element that will visualize the LLM call\n",
        "    llm_call_visualiser = LLMCallVisualiser(prompt=function_generation_instruction)\n",
        "    llm_call_visualiser.display()\n",
        "\n",
        "    llm_results = chat_completion(\n",
        "        prompt,\n",
        "        temperature=0.15,  # To make generations more diverse (creativity)\n",
        "        llm_call_visualiser=llm_call_visualiser)\n",
        "\n",
        "    # Here we only extract the generated function without imports section.\n",
        "    # That's ok for us, since we'll have a bug-fix stage, which will recover those.\n",
        "    generated_code = extract_code(\n",
        "        llm_results,\n",
        "        extract_only_funcs_and_classes=True,\n",
        "        remove_starting_code=True)\n",
        "\n",
        "    # Show generated code\n",
        "    if llm_call_visualiser is not None:\n",
        "        llm_call_visualiser.update_content(\n",
        "            tab_name=\"Generated Code\",\n",
        "            text=generated_code,\n",
        "            change_tab=True)\n",
        "\n",
        "    return llm_results, generated_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D868LnLrVptE"
      },
      "outputs": [],
      "source": [
        "print(f\"Generating function...\")\n",
        "\n",
        "# Initial function code generation is done here.\n",
        "llm_results, generated_code = generate_code(\n",
        "    prompt=function_generation_instruction)\n",
        "assert contains_function(generated_code), \"Function is missing!\"\\\n",
        "                                          \" Rerun the cell until assertion passes.\"\n",
        "\n",
        "# Load the code into global context by running it.\n",
        "# Generated function is not being called here, just defined.\n",
        "exec(generated_code, globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeXMPdWlVptE"
      },
      "source": [
        "# Creating unit tests using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoosHCwqVptF"
      },
      "outputs": [],
      "source": [
        "def create_unit_tests(\n",
        "    code_to_be_tested: str,\n",
        "    unit_testing_hints: str = None,\n",
        ") -> (str, str):\n",
        "    \"\"\"\n",
        "    Creating unit tests for code_to_be_tested taking into account unit_testing_hints.\n",
        "    \"\"\"\n",
        "\n",
        "    if unit_testing_hints is None:\n",
        "        unit_testing_hints = \"\"\n",
        "\n",
        "    prompt = \"\\n\".join([\n",
        "        f\"Write a perfectly runnable Test(unittest.TestCase) with several fully\"\n",
        "            f\" implemented test cases including edge cases.\",\n",
        "        f\"{unit_testing_hints}\",\n",
        "        f\"\",\n",
        "        f\"Function to be tested:\",\n",
        "        f\"{wrap_code(code_to_be_tested)}\",\n",
        "    ])\n",
        "\n",
        "    # Output UI element that will visualize the LLM call\n",
        "    llm_call_visualiser = LLMCallVisualiser(prompt=prompt)\n",
        "    llm_call_visualiser.display()\n",
        "\n",
        "    llm_results = chat_completion(\n",
        "        prompt,\n",
        "        temperature=0.15,\n",
        "        llm_call_visualiser=llm_call_visualiser)\n",
        "\n",
        "    # We are interested only in code within the answer\n",
        "    generated_code = extract_code(\n",
        "        llm_results,\n",
        "        extract_only_funcs_and_classes=True,\n",
        "        remove_starting_code=True)\n",
        "\n",
        "    # Show generated code\n",
        "    if llm_call_visualiser is not None:\n",
        "        llm_call_visualiser.update_content(\n",
        "            tab_name=\"Generated Code\",\n",
        "            text=generated_code,\n",
        "            change_tab=True)\n",
        "\n",
        "    # Make sure we return a concatenation of tested code + unit tests\n",
        "    if contains_function(generated_code):\n",
        "        # If LLM included tested code along with unit tests, then ok.\n",
        "        code_with_unit_tests = generated_code\n",
        "    else:\n",
        "        # Prepend the generated unit tests with the tested code.\n",
        "        code_with_unit_tests = f\"{code_to_be_tested}\\n\\n{generated_code}\"\n",
        "\n",
        "    return llm_results, code_with_unit_tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFeeKqUQVptF"
      },
      "outputs": [],
      "source": [
        "print(f\"Generating unit tests...\")\n",
        "\n",
        "# Initial unit tests code generation is done here.\n",
        "llm_results, code_with_unit_tests = create_unit_tests(\n",
        "    generated_code,\n",
        "    unit_testing_hints)\n",
        "assert contains_class(code_with_unit_tests), \"Unit tests class is missing!\"\\\n",
        "                                             \" Rerun the cell until assertion passes.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IE65I5KVptF"
      },
      "source": [
        "# Make sure there is at least 1 issue after running unit tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Qb-d9XVptF"
      },
      "outputs": [],
      "source": [
        "tests_were_runnable, unit_tests_issues = run_unit_tests(code_with_unit_tests, scope_namespace=globals())\n",
        "if len(unit_tests_issues) == 0:\n",
        "    # Introducing a simple bug by changing \"def\" to \"ef\"\n",
        "    # We need at least 1 bug to demonstrate the bug-fix\n",
        "    code_with_unit_tests = code_with_unit_tests[1:]\n",
        "    tests_were_runnable, unit_tests_issues = run_unit_tests(code_with_unit_tests, scope_namespace=globals())\n",
        "\n",
        "print(f\"Unit tests were runnable:\\n{'-' * 64}\\n{tests_were_runnable}\\n\\n\")\n",
        "print(f\"Issues count:\\n{'-' * 64}\\n{len(unit_tests_issues)}\\n\\n\")\n",
        "unit_tests_issues_str = \"\\n\\n\\n\".join(\n",
        "    [f\"{i}. {issue}\"\n",
        "     for i, issue in enumerate(unit_tests_issues, start=1)]) or \"None\"\n",
        "print(f\"Issues:\\n{'-' * 64}\\n{unit_tests_issues_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20F2hY9HVptF"
      },
      "source": [
        "# Try fixing bugs using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy-5UIugVptF"
      },
      "outputs": [],
      "source": [
        "def try_to_fix_bugs(\n",
        "    code: str,\n",
        "    code_issues: list[str],\n",
        "    generation_hints: str = None,\n",
        "    max_issues_shown_to_llm: int = 2,\n",
        ") -> (str, str):\n",
        "    \"\"\"\n",
        "    Trying to fix bugs with 1 LLM call\n",
        "    \"\"\"\n",
        "\n",
        "    code_issues_copy = code_issues.copy()\n",
        "    random.shuffle(code_issues_copy)\n",
        "    issues_shown_to_llm = code_issues_copy[:max_issues_shown_to_llm]\n",
        "    issues_shown_to_llm_str = \"\\n\\n\\n\".join(\n",
        "        [f\"{i}. {issue}\"\n",
        "         for i, issue in enumerate(issues_shown_to_llm, start=1)]) or \"None\"\n",
        "\n",
        "    if generation_hints is None:\n",
        "        generation_hints = \"\"\n",
        "\n",
        "    prompt = \"\\n\".join([\n",
        "        f\"Instruction:\",\n",
        "\n",
        "        # Here we encouraging model to think before generation (Chain-of-Thought)\n",
        "        #   which in general improves answers and allows you to see how LLM thinks.\n",
        "        f\"1. Analize failing test cases in plain text.\"\n",
        "            f\" Remember that sometimes bugs are within the test cases.\",\n",
        "\n",
        "        # This is to prevent LLM from skipping tests that succeed.\n",
        "        f\"2. Rewrite the whole Code including tested function and FULL implementations\"\n",
        "            f\" of ALL unit tests, so that all Tests Issues are resolved.\",\n",
        "\n",
        "        # To remember unit testing hints.\n",
        "        f\"{generation_hints}\",\n",
        "\n",
        "        f\"\",\n",
        "        f\"Code:\",\n",
        "        f\"{wrap_code(code)}\",\n",
        "        f\"\",\n",
        "        f\"Tests Issues ({len(issues_shown_to_llm)} out of {len(code_issues)}):\",\n",
        "        f\"```\",\n",
        "        f\"{issues_shown_to_llm_str}\",\n",
        "        f\"```\",\n",
        "        f\"\",\n",
        "\n",
        "        # # This is not to ignore steps from Instruction above.\n",
        "        # f\"Number each step from Instruction.\"\n",
        "    ])\n",
        "\n",
        "    # Output UI element that will visualize the LLM call.\n",
        "    llm_call_visualiser = LLMCallVisualiser(prompt=prompt)\n",
        "    llm_call_visualiser.display()\n",
        "\n",
        "    llm_results = chat_completion(\n",
        "        prompt,\n",
        "        temperature=0.15,\n",
        "        llm_call_visualiser=llm_call_visualiser)\n",
        "\n",
        "    if len(llm_results.split(\"```\")) % 2 != 1:\n",
        "        print(f\"Probably, fixed code is not complete (not all of ``` have pairs).\\n\")\n",
        "        tests_were_runnable = False\n",
        "        unit_tests_issues = [\"Valid code block wasn't found\"]\n",
        "    else:\n",
        "        candidate_code = extract_code(\n",
        "            llm_results,\n",
        "            extract_only_funcs_and_classes=False,\n",
        "            remove_starting_code=True)\n",
        "\n",
        "        # Show generated code\n",
        "        if llm_call_visualiser is not None:\n",
        "            llm_call_visualiser.update_content(\n",
        "                tab_name=\"Generated Code\",\n",
        "                text=candidate_code,\n",
        "                change_tab=True)\n",
        "\n",
        "        if not contains_function(candidate_code) or not contains_class(candidate_code):\n",
        "            tests_were_runnable = False\n",
        "\n",
        "            unit_tests_issues = []\n",
        "            if not contains_function(candidate_code):\n",
        "                unit_tests_issues.append(\"Fixed code misses tested function.\")\n",
        "            if not contains_class(candidate_code):\n",
        "                unit_tests_issues.append(\"Fixed code misses unit test definition.\")\n",
        "\n",
        "            print(\"\\n\".join(unit_tests_issues))\n",
        "        else:\n",
        "            code = candidate_code\n",
        "\n",
        "            tests_were_runnable, unit_tests_issues = run_unit_tests(\n",
        "                code,\n",
        "                scope_namespace=globals())\n",
        "            unit_tests_issues_str = \"\\n\\n\\n\".join(\n",
        "                [f\"{i}. {issue}\"\n",
        "                 for i, issue in enumerate(unit_tests_issues, start=1)]) or \"None\"\n",
        "\n",
        "            # Show unit tests issues\n",
        "            if llm_call_visualiser is not None:\n",
        "                llm_call_visualiser.update_content(\n",
        "                    tab_name=\"Code Issues\",\n",
        "                    text=unit_tests_issues_str,\n",
        "                    change_tab=(len(unit_tests_issues) > 0))\n",
        "\n",
        "    return llm_results, code, tests_were_runnable, unit_tests_issues\n",
        "\n",
        "def fix_bugs_in_several_attempts(\n",
        "    code_with_unit_tests: str,\n",
        "    tests_were_runnable: bool = None,\n",
        "    unit_tests_issues: list[str] = None,\n",
        "    generation_hints: str = None,\n",
        "    attempts_count: int = 20,\n",
        ") -> (str, list[str]):\n",
        "    \"\"\"\n",
        "    Trying to fix bugs iteratively using several attempts\n",
        "    \"\"\"\n",
        "\n",
        "    if tests_were_runnable is None or unit_tests_issues is None:\n",
        "        tests_were_runnable, unit_tests_issues = run_unit_tests(\n",
        "            code_with_unit_tests,\n",
        "            scope_namespace=globals())\n",
        "\n",
        "    # Initialize best known solution as the initial one\n",
        "    total_issues_count = len(unit_tests_issues) if tests_were_runnable else 1000000\n",
        "    total_issues_count_str = str(total_issues_count) if tests_were_runnable else \"Unit tests weren't runnable\"\n",
        "    best_code_with_unit_tests = code_with_unit_tests\n",
        "    best_unit_tests_issues = unit_tests_issues\n",
        "    best_issues_count = total_issues_count\n",
        "    best_issues_count_str = total_issues_count_str\n",
        "\n",
        "    print(f\"Initial number of unit tests issues: {total_issues_count_str}\\n\")\n",
        "\n",
        "    for attempt in range(1, attempts_count + 1):\n",
        "        if total_issues_count == 0:\n",
        "            print(\"All unit tests succeeded!\\n\")\n",
        "            break\n",
        "\n",
        "        print(f\"\\n{'=' * 4} Bug-fix attempt {attempt} out of {attempts_count} {'=' * 48}\")\n",
        "\n",
        "        llm_results, code_with_unit_tests, tests_were_runnable, unit_tests_issues = try_to_fix_bugs(\n",
        "            best_code_with_unit_tests,\n",
        "            best_unit_tests_issues,\n",
        "            generation_hints=generation_hints,\n",
        "            max_issues_shown_to_llm=2,  # We keep it small not to make the prompt huge\n",
        "        )\n",
        "\n",
        "        if tests_were_runnable:\n",
        "            # Update best known solution if results are not worse than previous one\n",
        "            total_issues_count = len(unit_tests_issues)\n",
        "            total_issues_count_str = str(total_issues_count)\n",
        "            if total_issues_count <= best_issues_count:\n",
        "                best_code_with_unit_tests = code_with_unit_tests\n",
        "                best_unit_tests_issues = unit_tests_issues\n",
        "                best_issues_count = total_issues_count\n",
        "                best_issues_count_str = str(best_issues_count)\n",
        "        else:\n",
        "            total_issues_count_str = \"Unit tests weren't runnable\"\n",
        "\n",
        "        print(f\"Current attempt unit tests issues: {total_issues_count_str};\"\n",
        "              f\" Best issues count so far: {best_issues_count_str}\\n\")\n",
        "\n",
        "    return best_code_with_unit_tests, best_unit_tests_issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVGxT1Q4VptF"
      },
      "outputs": [],
      "source": [
        "# Actual bug-fix starts here.\n",
        "# Be careful! Sometimes it might not be able to fix all issues (if so, you can try again\n",
        "#   starting from \"Initial code generation\" section).\n",
        "# Or even cheat by tuning the unit tests so that they pass on buggy code!\n",
        "# Also the code doesn't check if all required libs are installed.\n",
        "# Note: Most capable models like GPT-4 (as of 2024-04-15) can easily solve all of the examples.\n",
        "code_with_unit_tests, unit_tests_issues = fix_bugs_in_several_attempts(\n",
        "    code_with_unit_tests,\n",
        "    tests_were_runnable=tests_were_runnable,\n",
        "    unit_tests_issues=unit_tests_issues,\n",
        "    generation_hints=unit_testing_hints,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huJxS499VptG"
      },
      "source": [
        "\n",
        "Now you can go back to the **Select code generation task** section and try other examples by changing the *task_example_id* variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiP_fG9DVptG"
      },
      "source": [
        "# Summary\n",
        "\n",
        "- Code generation can be guided by textual description, defining constraints, showing input-output examples, etc.\n",
        "- Unit tests for code validation can be generated, but when possible, prefer prompting to use calculated expected values.\n",
        "- Generated code and unit tests can be iteratively improved by asking LLM to fix the issues.\n",
        "- Final code is not guaranteed to be bug-free even if all generated tests pass.\n",
        "- We covered only relatively simple functions. For more complex ones we might need more sophisticated bug-fixing flow (e.g. adding use of debug printing, passing previous solutions within the prompt, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFoguJuGVptG"
      },
      "source": [
        "# Homework\n",
        "\n",
        "1. Play with existing examples to see how it works without unit_testing_hints or modify function_generation_instruction.\n",
        "\n",
        "2. Using LLM generate json with countries, continents they are located at, their capitals and populations.\n",
        "\n",
        "3. Extend code_generation_tasks_examples above with a task to generate function, that loads generated json as a pandas DataFrame.\n",
        "\n",
        "4. Generate one more function to show resultant DataFrame as a bar chart with continents populations.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEtRSJyhVptG"
      },
      "source": [
        "# Other cool LLM stuff related to coding\n",
        "\n",
        "Here is the link to a comprehensive review of LLM researches for code: https://github.com/codefuse-ai/Awesome-Code-LLM.\n",
        "Initially it was introduced for the \"Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code\" paper (https://arxiv.org/abs/2311.07989), but is being updated frequently (last updated on 2024-04-15, which was yesterday as of writing).\n",
        "\n",
        "It contains references to papers/code/models for the most of popular code related tasks.\n",
        "Here is the **Table of Contents**:\n",
        "\n",
        "1. [Surveys](#1-surveys)\n",
        "\n",
        "2. [Models](#2-models)\n",
        "\n",
        "   2.1 [Off-the-Shelf LLM](#21-off-the-shelf-llm)\n",
        "\n",
        "   2.2 [Existing LLM Adapted to Code](#22-existing-llm-adapted-to-code)\n",
        "\n",
        "   2.3 [General Pretraining on Code](#23-general-pretraining-on-code)\n",
        "\n",
        "   - [Encoder](#encoder)\n",
        "   - [Decoder](#decoder)\n",
        "   - [Encoder-Decoder](#encoder-decoder)\n",
        "   - [UniLM](#unilm)\n",
        "\n",
        "   <!-- prettier ignore -->\n",
        "\n",
        "   2.4 [(Instruction) Fine-Tuning on Code](#24-instruction-fine-tuning-on-code)\n",
        "\n",
        "   2.5 [Reinforcement Learning on Code](#25-reinforcement-learning-on-code)\n",
        "\n",
        "3. [When Coding Meets Reasoning](#3-when-coding-meets-reasoning)\n",
        "\n",
        "   3.1 [Coding for Reasoning](#31-coding-for-reasoning)\n",
        "\n",
        "   3.2 [Code Simulation](#32-code-simulation)\n",
        "\n",
        "   3.3 [Coding via Planning](#33-coding-via-planning)\n",
        "\n",
        "   3.4 [Interactive Coding](#34-interactive-coding)\n",
        "\n",
        "4. [Code LLM for Low-Resource, Low-Level, and Domain-Specific Languages](#4-code-llm-for-low-resource-low-level-and-domain-specific-languages)\n",
        "\n",
        "5. [Methods/Models for Downstream Tasks](#5-methodsmodels-for-downstream-tasks)\n",
        "\n",
        "   - [Code Generation](#code-generation)\n",
        "   - [Code Translation](#code-translation)\n",
        "   - [Code Summarization](#code-summarization)\n",
        "   - [Program Repair](#program-repair)\n",
        "   - [Vulnerability Detection](#vulnerability-detection)\n",
        "   - [Type Prediction](#type-prediction)\n",
        "   - [Malicious Code Detection](#malicious-code-detection)\n",
        "   - [Repository-Level Coding](#repository-level-coding)\n",
        "   - [Compiler Optimization](#compiler-optimization)\n",
        "   - [Frontend Development & Web Agents](#frontend-development--web-agents)\n",
        "   - [Decompilation](#decompilation)\n",
        "   - [Test Generation](#test-generation)\n",
        "\n",
        "6. [Analysis of AI-Generated Code](#6-analysis-of-ai-generated-code)\n",
        "\n",
        "7. [User-LLM Interaction](#7-user-llm-interaction)\n",
        "\n",
        "8. [Datasets](#8-datasets)\n",
        "\n",
        "   8.1 [Pretraining](#81-pretraining)\n",
        "\n",
        "   8.2 [Benchmarks](#82-benchmarks)\n",
        "\n",
        "   - [Program Synthesis](#program-synthesis)\n",
        "   - [Text-to-SQL](#text-to-sql)\n",
        "   - [Code Translation](#code-translation-1)\n",
        "   - [Program Repair](#program-repair-1)\n",
        "   - [Code Summarization](#code-summarization-1)\n",
        "   - [Defect/Vulnerability Detection](#defectvulnerability-detection)\n",
        "   - [Code Retrieval](#code-retrieval)\n",
        "   - [Type Inference](#type-inference)\n",
        "   - [Commit Message Generation](#commit-message-generation)\n",
        "   - [Repo-Level Coding](#repo-level-coding)\n",
        "\n",
        "9. [Recommended Readings](#9-recommended-readings)\n",
        "\n",
        "10. [Citation](#citation)\n",
        "\n",
        "11. [Star History](#star-history)\n",
        "\n",
        "12. [Join Us](#join-us)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8Pln_OMst0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}